<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>es 基础概念</title>
    <url>/2023/02/3fe4a59a/</url>
    <content><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><p>es 主旨是随时可用和按需扩容</p>
<p>es 垂直扩容是有限的，因为单机性能有极限，故 es 真正的扩容能力来自水平扩容</p>
<p>es 天生就是分布式的</p>
<h2 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h2><p>一个运行中的 es 示例称为一个节点，集群是由一个或多个 cluster.name 配置一致的节点组成。当节点加入或从集群剔除后，集群会重新平均分配所有节点的数据。</p>
<p>当一个节点被选举为主节点后，该主节点将负责管理集群层面的所有变更，如增删索引、增删节点等。但是主节点不会涉及文档级别的变更和搜索操作。</p>
<p>用户可以将请求发送到集群中的任何节点，包括主节点。每个节点都知道任意文档的存储位置，并且能将请求转发到文档存储所在的节点，同时任意节点都能负责从各个包含所需数据的节点中查询、收集数据，并将结果集返回给客户端。</p>
<p>为了实现故障转移，集群节点数量需要大于等于主分片数量，当启动新的节点加入集群时，节点会根据 cluster.name 自动发现集群并加入。但是在不同的机器上启动节点时，为了加入同一集群，需要配置一个可连接到的单播主机列表 discovery.zen.ping.unicast.hosts</p>
<p>单播模式是一种网络通讯中点对点的数据传输模式，es 通过配置记住列表，使用单播模式完成节点通讯。</p>
<blockquote>
<p>discovery.zen.ping.unicast.hosts 不需要配置集群上的所有节点。因为如果一个节点连接到了集群中的一个节点，这个连接信息会被发送到集群中的所有其它节点，如果一个节点保持了太多并发网络连接将导致额外的系统负载。所以通过配置 discovery.zen.ping.unicats.concurrent_connects 约束了单播模式的最大网络连接数，默认值 10。</p>
</blockquote>
<h2 id="分片"><a href="#分片" class="headerlink" title="分片"></a>分片</h2><p>分片是一个功能完整的搜索引擎，它拥有使用节点上所有资源的能力。</p>
<p>因为每个分片都可以使用节点上的所有资源，所以理论上一个分片独占一个节点是最好的。故水平扩容（增加集群节点）也是常见的提高负载能力的操作。</p>
<p>因为主分片的数目在索引创建的时候已经确定下来了，而实际上主分片数量定义了索引能够存储的最大数据量（实际上还要考量数据、硬件等）。但是因为读操作可以同时被主分片和副本分片处理，所以拥有越多的副本分片集群的吞吐量越高。</p>
<p>综上，集群的副本分片越多，分片越能独占节点资源，集群的吞吐量越高。</p>
<p>归根到底就是节点数量越多越好（废话，哈哈）</p>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>索引实际上是指向一个或多个物理分片的逻辑命名空间。</p>
<p>分片是 es 底层的工作单元，用于保存索引数据的一部分。每一个分片都是一个 Lucene 实例，它本身也是一个完整的搜索引擎。</p>
<p>文档被存储到分片中，应用程序直接与索引交互完成操作，索引再操作分片。</p>
<p>索引中任一文档都归属于一个确定的主分片，所以主分片的数量决定索引能存储的文档数量。</p>
<p>副本分片只是主分片内容的拷贝，副本分片作为硬件故障时保护数据不丢失的冗余备份，同时提供查询服务。</p>
<p>索引在建立时就已经确定主分片数量，副本分片数量则可以随时修改。</p>
<p>在同一个节点上同时保存原始数据和副本是无意义的，因为一旦该节点故障就会同时失去原始数据和副本，所谓保护数据不丢失就不存在了。</p>
<h2 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h2><p>创建一个名为 index 主分片和副本分片个数都是 1 的索引</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PUT /&#123;index&#125;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;number_of_shards&quot;:1,</span><br><span class="line">        &quot;number_of_replicas&quot;:1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h1 id="集群健康"><a href="#集群健康" class="headerlink" title="集群健康"></a>集群健康</h1><p>集群健康主要通过接口 &#x2F;_cluster&#x2F;health 返回的 status 字段判断，枚举值：green，yellow，red。</p>
<p>green：集群健康</p>
<p>yellow：所有主分片都正常，但至少一个副本分片出现了问题</p>
<p>red：有主分片出现了问题</p>
<blockquote>
<p>注意：描述集群健康时衡量的主体是分片（主分片和副分片），而不是节点。</p>
</blockquote>
<p>KVM 测试环境示例：</p>
<p><a href="http://192.168.20.213:9200/_cluster/health?pretty">http://192.168.20.213:9200/_cluster/health?pretty</a></p>
<p>结果：</p>
<img src="../../images/image-20221205151152327.png" alt="image-20221205151152327" style="zoom:50%;" />





<h1 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h1><p>集群扩容的方式有水平扩容和垂直扩容。</p>
<p>水平扩容就是通过启动更多的节点来分散负载，达到提高性能目的的手段，当然如果节点都在一个机器上是无意义的。</p>
<p>垂直扩容就是将 4H8G 升级为 8H16G。</p>
<p>好像没啥好记录的，就是一个加机器，一个升配置。</p>
<h1 id="应对故障"><a href="#应对故障" class="headerlink" title="应对故障"></a>应对故障</h1><p>假设一个集群拥有三个节点，主分片数量为 3，副本分片数量为 2（也就是每个主分片拥有 2 个副本分片），当其中某一个节点异常推出集群，会发生什么呢？</p>
<p>因为节点数等于主分片数，所以主分片应该均匀分布在三个节点汇总，当其中一个节点异常退出，说明我们损失了一个主分片和两个副本分片。</p>
<p>如果退出的节点刚刚好是集群的主节点，集群首先会选举一个新的主节点。</p>
<p>然后主节点会检查集群情况，发现丢失了一个主分片，索引不能正常工作，集群的状态变更为 red。</p>
<p>此时集群的数据因为冗余的缘故，并没有丢失数据。主节点将其它节点中的副本分片提升为主分片，重新构建集群分片关系，这个过程是瞬间的，如同按下一个开关。然后集群的状态变更为 yellow。</p>
<blockquote>
<p>此时的状态 yellow 是因为我们的集群主分片依然完备，但是当前集群只有两个节点，只能满足每个主分片拥有一个副本分片，与配置 number_of_replicas&#x3D;2 不符，也就是说集群中存在副本分片未达预期。</p>
<p>一个节点同时保存相同数据的主分片和副本分片是无意义的。同时保存一个主分片的两个相同副本分片也是无意义的。</p>
</blockquote>
<p>如果此时第二个节点也异常推出了，集群状态依然是 yellow，因为第三个节点中的副本分片都将升级为主分片，集群数据依然无损。</p>
<p>最后是等待开发人员介入了，如果退出的节点重新回归集群，且该节点依然拥有之前的分片，集群会尝试重用它们，同时仅从主分片复制发生了修改的数据文件过去。然后集群会尝试再次优化所有节点上的分片分布。</p>
<h1 id="疑点"><a href="#疑点" class="headerlink" title="疑点"></a>疑点</h1><ol>
<li>这句话没理解</li>
</ol>
<img src="../../images/image-20221205150325735.png" alt="image-20221205150325735" style="zoom:50%;" />

<p>暂时理解的意思是：只有一个主节点的时候因为没有数据节点，所以压根不存储数据，也就不存在文档级别操作所以不会存在所谓瓶颈</p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>es 数据输入和输出</title>
    <url>/2023/02/f0c5d84/</url>
    <content><![CDATA[<h1 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h1><p>es 使用 json 存储文档信息。</p>
<p>一般情况下，可以理解为文档就是存储对象。</p>
<p>实际上有微小从差异，对象是类似 hash、hashMap、字典等的 json 对象，对象可以包含或嵌套其它对象。而在 es 中，文档指最顶层的根对象，这个根对象被序列化成 json 并存储到 es 中，指定了唯一文档 ID。</p>
<p>json 对象中的属性可以是任何合法字符串，但不能包含英文句号。</p>
<p>es 文档中每个属性都有为了快速检索设置的倒排索引。</p>
<h2 id="文档元数据"><a href="#文档元数据" class="headerlink" title="文档元数据"></a>文档元数据</h2><p>文档除了传入的对象数据以外，还有一些记录文档自身信息的数据，称为元数据。元数据字段以下划线开头</p>
<p>常见元数据有：</p>
<p>_index：文档存放的索引名（存疑）</p>
<p>_type：文档表示的对象类别，es 允许在索引中对数据进行逻辑分区。不同的 type 的文档可以拥有不同的字段，但是最好高度相似</p>
<p>_id：文档唯一标识</p>
<p>_version：标记文档版本的属性，文档每次被修改后（包括删除）都会递增，_version 可以用来确保应用程序的一部分修改不会覆盖另一部分所做的修改</p>
<p>_source：保存我们传递给 es 的原始数据的属性</p>
<h1 id="文档操作"><a href="#文档操作" class="headerlink" title="文档操作"></a>文档操作</h1><h2 id="增"><a href="#增" class="headerlink" title="增"></a>增</h2><h3 id="自定义-ID-创建文档"><a href="#自定义-ID-创建文档" class="headerlink" title="自定义 ID 创建文档"></a>自定义 ID 创建文档</h3><figure class="highlight text"><table><tr><td class="code"><pre><span class="line"># 向 &#123;index&#125; 索引的类型 &#123;type&#125; 中添加一条 id 为 &#123;id&#125; 的文档</span><br><span class="line">PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;</span><br><span class="line">&#123;</span><br><span class="line">	&quot;field&quot;:&quot;value&quot;,</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述方式是没有该 ID 值时新建文档，存在时更新，就是 mysql 的 update or create。</p>
<p>如果只希望新建，可以使用以下两种办法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 通过指定 op_type 参数值 create</span><br><span class="line">PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;?op_type=create</span><br><span class="line">&#123;</span><br><span class="line">	&quot;field&quot;:&quot;value&quot;,</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># URL 末端添加 _create</span><br><span class="line">PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_create</span><br><span class="line">&#123;</span><br><span class="line">	&quot;field&quot;:&quot;value&quot;,</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这两种方式，如果在 ID 已存在时，HTTP 响应码是 409 Conflict。</p>
<h3 id="默认自动-ID-创建文档"><a href="#默认自动-ID-创建文档" class="headerlink" title="默认自动 ID 创建文档"></a>默认自动 ID 创建文档</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 注意这里是 POST 而不是 PUT</span><br><span class="line">POST /&#123;index&#125;/&#123;type&#125;</span><br><span class="line">&#123;</span><br><span class="line">	&quot;field&quot;:&quot;value&quot;,</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="删"><a href="#删" class="headerlink" title="删"></a>删</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">DELETE /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;</span><br></pre></td></tr></table></figure>

<p>如果该文档存在，将正常删除文档并返回 HTTP code 200，同时返回的信息中文档的 _version 变更。</p>
<p>如果文档不存在，将得到 HTTP code 404，同时返回的响应体形如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;found&quot; :    false,</span><br><span class="line">  &quot;_index&quot; :   &quot;website&quot;,</span><br><span class="line">  &quot;_type&quot; :    &quot;blog&quot;,</span><br><span class="line">  &quot;_id&quot; :      &quot;123&quot;,</span><br><span class="line">  &quot;_version&quot; : 4</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意即使文档不存在，_version 也会增加，这是 es 内部记录标识的一部分，用来确保这些改变在跨多节点时以正确的顺序执行，避免在并发的情况下（瞬间先删后增和先增后删）导致与预期不一致的结果。</p>
<h3 id="改"><a href="#改" class="headerlink" title="改"></a>改</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;</span><br><span class="line">&#123;</span><br><span class="line">	&quot;field&quot;:&quot;new value&quot;,</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个请求跟自定义 ID 新建文档是一致的，区别在 es 的返回信息中。如果修改成功，返回信息中的 _version 值将得到递增，如 1-&gt;2，同时 created 属性值变成 false。</p>
<p>虽然视觉上是对一条文档进行了更新，但实际上在 es 的底层执行了如下流程：</p>
<ol>
<li>查询旧文档并构建 json</li>
<li>更新 json 中的属性</li>
<li>删除旧文档</li>
<li>索引（新建）一个新的文档</li>
</ol>
<blockquote>
<p>注意，es 的删除文档也不是真的就立刻物理删除，只是不能再访问已删除文档而已。es 在后台有额外的删除服务清理这些废弃文档。</p>
<p>上述步骤均发生在主分片内部，相比手动模拟更新节省网络开销，也减少并发带来的冲突可能性。</p>
</blockquote>
<h3 id="更新的并发问题"><a href="#更新的并发问题" class="headerlink" title="更新的并发问题"></a>更新的并发问题</h3><p>当短时间内有多个客户端尝试修改 es 中同一条文档的信息时，文档最终将表现为最后一个更新的内容，其余更新都将丢失。这里我理解是，更新操作的 GET 和 DELETE、CREATE 操作是原子性的，所以多个更新操作中的 GET _version 总是能体现出大小区别，进而 es 将以 _version 更大的更新操作结果为准。（不是很确定是不是这个理解）</p>
<p>在数据库领域，处理并发冲突通常有两种思路，一种是悲观并发控制，就是通过锁机制来保证同时只能一个角色修改它；另一个则是 es 使用的乐观并发控制，这个思路假定冲突不会发生，并且不会阻塞尝试更新的操作。然而如果源数据在读写中被修改，更新将失败并返回信息，将失败后如何解决冲突交给应用程序。（这里感觉有点前后冲突，查询文档并不会更新 _version 啊）</p>
<p>es 使用 _version 实现乐观并发控制。因为 _version 会在文档更新时递增，故 es 通过判断 _version 大小辨别新旧文档版本，当旧版本的文档在新版本之后达到，它将可以被简单地忽略。</p>
<h3 id="失败重试"><a href="#失败重试" class="headerlink" title="失败重试"></a>失败重试</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POST /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_update?retry_on_conflict=5</span><br><span class="line">&#123;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过请求参数 retry_on_conflict 可以设置 update 并发冲突时自动重试的次数。</p>
<h3 id="按照版本号更新文档"><a href="#按照版本号更新文档" class="headerlink" title="按照版本号更新文档"></a>按照版本号更新文档</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 只在文档 _version=1 时成功更新，其余时候返回 409 Conflict</span><br><span class="line">PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;?version=1</span><br></pre></td></tr></table></figure>

<h3 id="使用外部字段完成版本控制"><a href="#使用外部字段完成版本控制" class="headerlink" title="使用外部字段完成版本控制"></a>使用外部字段完成版本控制</h3><p>常见的情景是使用外部数据库作为存储主库，es 作为数据检索的扩展库。这意味着主库发生更改的数据需要被复制到 es，而且数据记录的版本控制都不是依托 es 实现的。此时就需要 es 能使用外部库的属性来管理自身 _version。</p>
<p>如果外部数据库已经有了版本号，或者其它能作为版本号的字段值（如 timestamp 类型），可以在请求中加入 version_type&#x3D;external 来指定来自外部的数据版本号。如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 创建/更新文档时，指定新的 _version 值为 1</span><br><span class="line">PUT /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;?version=1&amp;version_type=external</span><br></pre></td></tr></table></figure>

<blockquote>
<p>外部版本号可以在创建、更新、删除文档时使用</p>
<p>外部版本号必须是大于 0，小于 9.2E+18（一个 java 的 long 值） 的整数</p>
</blockquote>
<p>更新时，操作传入的外部版本号也必须比当前 es 记录的版本号更大，否则同样得到 HTTP code 409。</p>
<h3 id="部分更新文档"><a href="#部分更新文档" class="headerlink" title="部分更新文档"></a>部分更新文档</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POST /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_update</span><br><span class="line">&#123;</span><br><span class="line">	&quot;doc&quot;: &#123;</span><br><span class="line">		&quot;new field&quot;:&quot;new value&quot;,</span><br><span class="line">		...</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述操作将为文档内容中添加 new filed 字段</p>
<h3 id="使用脚本更新文档"><a href="#使用脚本更新文档" class="headerlink" title="使用脚本更新文档"></a>使用脚本更新文档</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 使符合条件的文档的 views 字段 +1</span><br><span class="line">POST /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_update</span><br><span class="line">&#123;</span><br><span class="line">	&quot;script&quot;:&quot;ctx._source.views+=1&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于 API 不能满足需求的情况，es 允许使用脚本编写自定义的逻辑。默认的脚本语言是 Groovy。可以通过配置 script.groovy.sandbox.enabled 来控制是否允许动态脚本（上述 API 传入脚本逻辑），打开将为 es 服务架构引入新的风险点。</p>
<p>不过关系上述配置以后，依然可以从每个节点的 config&#x2F;scripts&#x2F; 目录下读取并运行 groovy 脚本。</p>
<p>以下是可使用脚本的其它例子：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 为文档的 tags 字段（数组类型）添加一个值 pokemon</span><br><span class="line">POST /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_update</span><br><span class="line">&#123;</span><br><span class="line">	&quot;script&quot;:&quot;ctx._source.tags+=new_tag&quot;,</span><br><span class="line">	&quot;params&quot;: &#123;</span><br><span class="line">		&quot;new_tag&quot;: &quot;pokemon&quot;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 删除文档字段 views 值为 1 的文档</span><br><span class="line">POST /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_update</span><br><span class="line">&#123;</span><br><span class="line">	&quot;script&quot;:&quot;ctx.op.tags = ctx._source.views == count ? &#x27;delete&#x27; : &#x27;none&#x27;&quot;,</span><br><span class="line">	&quot;params&quot;: &#123;</span><br><span class="line">		&quot;count&quot;: 1</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用脚本也可以执行 update or create 操作，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POST /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_update</span><br><span class="line">&#123;</span><br><span class="line">	&quot;script&quot;:&quot;ctx._source.views+=1&quot;</span><br><span class="line">	&quot;upsert&quot;: &#123;</span><br><span class="line">		&quot;count&quot;: 1</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通过 upset 的设置，当文档不存在时将创建一条 views 值为 1 的文档。当文档存在时将执行 script 中的逻辑进行 +1。</p>
<h2 id="查"><a href="#查" class="headerlink" title="查"></a>查</h2><h3 id="按照-ID-查询文档"><a href="#按照-ID-查询文档" class="headerlink" title="按照 ID 查询文档"></a>按照 ID 查询文档</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;?pretty</span><br></pre></td></tr></table></figure>

<p>查询请求中带上 pretty 参数可以调用 es 的 pretty-print 功能，可以提高 json 响应体可读性</p>
<p>如果查询不存在的 ID 的文档，请求 HTTP 响应码是 404 NOT FOUND，所以这个方式也可以用来判断文档是否存在，当然如果只是需要判断存在，用 HEAD 代替 GET 也是可以的。</p>
<p>默认情况下，GET 请求会返回整儿个文档</p>
<h3 id="只查询文档中个别字段"><a href="#只查询文档中个别字段" class="headerlink" title="只查询文档中个别字段"></a>只查询文档中个别字段</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;?_source=title,text</span><br></pre></td></tr></table></figure>

<p>_source 中，只提取了文档中的 title 和 text 属性返回</p>
<h3 id="只查询文档信息，排除元数据"><a href="#只查询文档信息，排除元数据" class="headerlink" title="只查询文档信息，排除元数据"></a>只查询文档信息，排除元数据</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET /&#123;index&#125;/&#123;type&#125;/&#123;id&#125;/_source</span><br></pre></td></tr></table></figure>

<p>注意这里 _source 不是作为参数传递，而是路径</p>
<h3 id="一次执行多个查询"><a href="#一次执行多个查询" class="headerlink" title="一次执行多个查询"></a>一次执行多个查询</h3><p>es 中可以使用 multi-get 或者 mget 将多个查询请求合并在一起发送，以减少当个请求查询导致的网络资源开销。</p>
<p>mget 中，通过在请求参数中增加 docs 层级，下级含括各个查询的查询条件完成。es 响应的结构体中也由 docs 包含对每个查询的返回结果，顺序与请求一致。</p>
<blockquote>
<p>使用 mget 批量执行查询时，只要请求正常都将得到 HTTP code 200，区别单个查询当文档不存在时的 404。批量执行查询时，个别查询的文档不存在，将在响应体的对应结果中反馈 found &#x3D; false。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># docs 理解为多维数组结构，每个子数组是一个独立的查询条件</span><br><span class="line">GET /_mget</span><br><span class="line">&#123;</span><br><span class="line">	&quot;docs&quot;: [</span><br><span class="line">		&#123;</span><br><span class="line">			&quot;_index&quot;: &quot;aaa&quot;</span><br><span class="line">		&#125;,</span><br><span class="line">		&#123;</span><br><span class="line">			&quot;_index&quot;: &quot;bbb&quot;,</span><br><span class="line">			&quot;_type&quot;: &quot;BBB&quot;</span><br><span class="line">		&#125;</span><br><span class="line">	]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果所有的子查询都同属一个索引（和类型），可以在请求路径上体现</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET /&#123;index&#125;/&#123;type&#125;/_mget</span><br><span class="line">&#123;</span><br><span class="line">	&quot;docs&quot;: [</span><br><span class="line">		&#123;</span><br><span class="line">			&quot;name&quot;: &quot;zhang san&quot;</span><br><span class="line">		&#125;,</span><br><span class="line">		&#123;</span><br><span class="line">			&quot;name&quot;: &quot;li si&quot;</span><br><span class="line">		&#125;</span><br><span class="line">	]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述情景下，也可以简化成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 这种知道就算了，还是 docs 吧，免得记混淆</span><br><span class="line">GET /&#123;index&#125;/&#123;type&#125;/_mget</span><br><span class="line">&#123;</span><br><span class="line">	&quot;name&quot;: [&quot;zhang san&quot;, &quot;li si&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="批量操作"><a href="#批量操作" class="headerlink" title="批量操作"></a>批量操作</h2><p>与使用 mget 批量查询操作类似，es 支持使用 bulk API 在单个请求中进行多次增删改查操作。</p>
<p>bulk 操作每一行代表一个操作，每个操作之前互相独立运行。</p>
<p>使用 bulk 操作需要遵循以下规则：</p>
<ol>
<li>每行必须以换行符（\n）结尾，包括最后一行</li>
<li>行内容不能包含未转义的换行符，这意味着这个 json 不能使用 pretty 参数打印。</li>
</ol>
<p>格式为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;action:&#123;metadata&#125;&#125;\n</span><br><span class="line">&#123;request body&#125;\n</span><br><span class="line">&#123;action:&#123;metadata&#125;&#125;\n</span><br><span class="line">&#123;request body&#125;\n</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>上述第一行中 action 指定该行将要执行哪些操作，必须是 create、index、update、delete 之一。它们的含义分别是：</p>
<ul>
<li>create：如果文档不存在就创建之。</li>
<li>index：创建或更新一个文档。</li>
<li>update：更新一个文档。</li>
<li>delete：删除一个文档。</li>
</ul>
<p>metadata 则用来指定被操作的文档的 _index、_type 和 _id。</p>
<p>上述第二行 request body 则是第一行的补充，是一个请求的请求体，比如 create 操作要创建的文档内容。</p>
<blockquote>
<p>request body 不是固定有的，delete 操作后面就不能跟着请求体，它后面只能是下一个 action 操作</p>
</blockquote>
<p>下述是一个完整的 bulk 请求示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POST /_bulk</span><br><span class="line">&#123;&quot;delete&quot;: &#123;&quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;&#125;&#125; </span><br><span class="line">&#123;&quot;create&quot;: &#123;&quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;&#125;&#125;</span><br><span class="line">&#123;&quot;title&quot;: &quot;My first blog post&quot;&#125;</span><br><span class="line">&#123;&quot;index&quot;:  &#123;&quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;&#125;&#125;</span><br><span class="line">&#123;&quot;title&quot;: &quot;My second blog post&quot;&#125;</span><br><span class="line">&#123;&quot;update&quot;: &#123;&quot;_index&quot;: &quot;website&quot;, &quot;_type&quot;: &quot;blog&quot;, &quot;_id&quot;: &quot;123&quot;, &quot;_retry_on_conflict&quot;: 3&#125;&#125;</span><br><span class="line">&#123;&quot;doc&quot;: &#123;&quot;title&quot;: &quot;My updated blog post&quot;&#125;&#125; </span><br></pre></td></tr></table></figure>



<p>bulk 操作的返回同样按照传递操作参数的顺序返回。items 下一个子结构体就是一个子操作的响应。结构形如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	&quot;took&quot;: 3,</span><br><span class="line">	&quot;errors&quot;: true, </span><br><span class="line">	&quot;items&quot;: [</span><br><span class="line">		&#123;&quot;create&quot;: &#123;</span><br><span class="line">			&quot;_index&quot;: &quot;website&quot;,</span><br><span class="line">            &quot;_type&quot;: &quot;blog&quot;,</span><br><span class="line">            &quot;_id&quot;: &quot;123&quot;,</span><br><span class="line">            # HTTP CODE</span><br><span class="line">            &quot;status&quot;: 409, </span><br><span class="line">            # 失败原因</span><br><span class="line">            &quot;error&quot;: &quot;DocumentAlreadyExistsException </span><br><span class="line">                        [[website][4] [blog][123]:</span><br><span class="line">                        document already exists]&quot;</span><br><span class="line">		&#125;&#125;,</span><br><span class="line">		&#123;&quot;index&quot;: &#123;</span><br><span class="line">			&quot;_index&quot;: &quot;website&quot;,</span><br><span class="line">            &quot;_type&quot;: &quot;blog&quot;,</span><br><span class="line">            &quot;_id&quot;: &quot;123&quot;,</span><br><span class="line">            &quot;_version&quot;: 5,</span><br><span class="line">            # HTTP CODE 互不影响</span><br><span class="line">            &quot;status&quot;: 200 </span><br><span class="line">		&#125;&#125;</span><br><span class="line">	]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果任一操作失败，在返回结构顶层的 error 将标记为 true。</p>
<h3 id="避免重复指定-index-和-type"><a href="#避免重复指定-index-和-type" class="headerlink" title="避免重复指定 index 和 type"></a>避免重复指定 index 和 type</h3><p>当批量操作都是针对相同的索引（和类型）时，bulk 和 mget 类似，也可以在请求路径中体现，如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POST /&#123;index&#125;/&#123;type&#125;/_bulk</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>路径中的 index 和 type 将被视为默认值，自动添加到省略号代表的每行 action 操作条件中，当然也可以自定义为其它的值来取缔路径默认值。</p>
<h3 id="批量操作的大小"><a href="#批量操作的大小" class="headerlink" title="批量操作的大小"></a>批量操作的大小</h3><p>负载的节点需要将整个批量操作请求加载到内存中处理，所以批量操作不是越多越好。具体最佳值视实际情况测试确定，一般一个批量请求占用物理内存 5~15M 是合适的。</p>
<h1 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h1><ol>
<li>文档元数据中的 _index 是指文档所在的索引名吗？<ol>
<li>是的</li>
</ol>
</li>
<li>文档的唯一标识是指 _id 还是 _index，_type，_id 三者的组合？<ol>
<li>后者</li>
</ol>
</li>
<li>更新的并发冲突，还不是很理解到底失败的是哪个，对更新冲突时的处理和最终结果理解还是不够<ol>
<li>现在看来自己将 update API 的操作理解为原子性是不合适的，在另一个文档有另一个解释</li>
<li><img src="/../../images/image-20221207112153824.png" alt="image-20221207112153824"></li>
<li>这个意思至少说明 update API 内部检索和新建之间是存在时间差的，甚至可能挤进来一个新的完整 update 操作。</li>
<li>这里的意思是检索得到文档的 _version 后，重新索引阶段，会判断 _version 值是否跟现有（为什么不是用 es 内部对文档的删除状态来判断？）记录的 _version 值作比较吗，如果不符合将操作失败。模拟一下，假设有一条文档 _verison&#x3D;5，同时来了 A&#x2F;B 两个相同的 update 请求，A 先检索到文档得到 _version&#x3D;5，但是在 A 重新索引文档前，B 完成了它的检索和重新索引操作（那么对 B 来说 _version 应该也是 5-&gt;6），然后 A 重新索引时执行的操作类似于 update … where _version &#x3D; 5 and … 这样的话确实（不对，既然 update 操作是分步走，而且到了重新索引的步骤，应该是 insert 而不是 update 了）<ol>
<li>那就是上面假设有问题了，也就是说 B._version 不是 5-&gt;6，可能是 6-&gt;7，5-&gt;6 是 A 触发的？可是 A 还没索引完成呢，怎么就被 B 请求获得了，这不是脏读了嘛</li>
<li>不管如何肯定的是 A 在重新索引操作时，依然有判断 _version 的逻辑，</li>
</ol>
</li>
<li>行吧，先放一下，这个问题还有很多自己没清楚的细节。核心问题变成了_version 是怎么帮助解决冲突的</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>filebeat 和 logstash 的关系</title>
    <url>/2023/02/53c6448/</url>
    <content><![CDATA[<p>在了解 ELK 的架构过程中，发现我们的服务部署还是用到了 filebeat，顺带也一并了解下，然后就发现貌似 filebeat 和 logstash 都是负责收集日志的，这下就很有必要深入了解下了。</p>
<h1 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h1><p>看到知乎有个很有意思的比喻：</p>
<p>filebeat 和 logstash 的关系就像环卫工人和垃圾车，垃圾车自己也可以自动将垃圾倒上车，但是会吵到你，有环卫工人配合的话就不会打扰你了。</p>
<p>目前学习的程度看应该是很准确的比喻。</p>
<h1 id="历史渊源"><a href="#历史渊源" class="headerlink" title="历史渊源"></a>历史渊源</h1><p>关于这两个工具的开发者也是有故事的，logstash 的作者原本为了优化 logstash 的日志采集写了个新的 logstash-forwarder，加入 elastic 公司以后，将 logstash-forwarder 和公司内部的另一个项目 packetbeat 合并后命名为 filebeat，从此 filebeat 成为 beats 的一员。</p>
<h1 id="架构关系"><a href="#架构关系" class="headerlink" title="架构关系"></a>架构关系</h1><p>另外下面是网上找到的一张关于这几个服务的架构关系图。</p>
<p><img src="/../../images/image-20230202145401390.png" alt="image-20230202145401390"></p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>filebeat</tag>
        <tag>kibana</tag>
      </tags>
  </entry>
  <entry>
    <title>gitbook 执行出错处理</title>
    <url>/2023/02/38c89d47/</url>
    <content><![CDATA[<h1 id="TypeError-cb-apply-is-not-a-function"><a href="#TypeError-cb-apply-is-not-a-function" class="headerlink" title="TypeError: cb.apply is not a function"></a>TypeError: cb.apply is not a function</h1><p>某一天回去写 gitbook，挥洒完汗水准备构建上线，发现自己 gitbook 环境歇菜了。</p>
<span id="more"></span>

<p>原因是最近在搞 hexo 怒将以前的旧版本 node V10 给删了，导致 gitbook build 出错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">➜  gitbook_local gitbook build</span><br><span class="line">/Users/hewenqian/.nvm/versions/node/v19.5.0/lib/node_modules/gitbook-cli/node_modules/npm/node_modules/graceful-fs/polyfills.js:287</span><br><span class="line">      if (cb) cb.apply(this, arguments)</span><br><span class="line">                 ^</span><br><span class="line"></span><br><span class="line">TypeError: cb.apply is not a function</span><br><span class="line">    at /Users/hewenqian/.nvm/versions/node/v19.5.0/lib/node_modules/gitbook-cli/node_modules/npm/node_modules/graceful-fs/polyfills.js:287:18</span><br><span class="line">    at FSReqCallback.oncomplete (node:fs:191:5)</span><br><span class="line"></span><br><span class="line">Node.js v19.5.0</span><br></pre></td></tr></table></figure>



<p>当前环境</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  gitbook_local gitbook -V</span><br><span class="line">CLI version: 2.3.2</span><br><span class="line">GitBook version: 3.2.3</span><br><span class="line"></span><br><span class="line">➜  gitbook_local node -v</span><br><span class="line">v19.5.0</span><br><span class="line"></span><br><span class="line">➜  gitbook_local npm --version</span><br><span class="line">9.4.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>网上的类似问题很多，大体可以归纳出结论：gitbook 3.2.3 只能适用在 node 10 以下。有两个解决思路，一：重装旧版本环境或 gitbook，二：就地解决！</p>
<p>自己不是用 nvm 嘛，把 node v10 装回来应该就能解决问题。只是自己用 node 只是需要，注意力不想放在这里，那就得尝试就地解决了。</p>
<p>vim &#x2F;Users&#x2F;hewenqian&#x2F;.nvm&#x2F;versions&#x2F;node&#x2F;v19.5.0&#x2F;lib&#x2F;node_modules&#x2F;gitbook-cli&#x2F;node_modules&#x2F;npm&#x2F;node_modules&#x2F;graceful-fs&#x2F;polyfills.js 检查调用，将该文件的第 62-64 行注释掉，亲测有效！</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// fs.stat = statFix(fs.stat)</span><br><span class="line">// fs.fstat = statFix(fs.fstat)</span><br><span class="line">// fs.lstat = statFix(fs.lstat)</span><br></pre></td></tr></table></figure>



<p>参考：</p>
<p><a href="https://stackoverflow.com/questions/63214997/typeerror-cb-apply-is-not-a-function">解决问题参考</a></p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>gridea 对接百度&amp;谷歌统计</title>
    <url>/2023/02/e8fb5eb8/</url>
    <content><![CDATA[<blockquote>
<p>谁不好奇自己的博客王章每天有没有访问量呢，嘻嘻</p>
</blockquote>
<h2 id="百度统计"><a href="#百度统计" class="headerlink" title="百度统计"></a>百度统计</h2><ol>
<li>登录到百度统计</li>
<li>在账户设置的【网站列表】添加自己的博客网站</li>
</ol>
<p><img src="/../../images/image-20230130171829973.png" alt="image-20230130171829973"></p>
<ol start="3">
<li><p>网站添加完以后在网站列表的操作列，点击【获取代码】</p>
</li>
<li><p>如果是自建网站，就将整块 js 代码复制贴到自己网站下。gridea 的话只需要提取其中的 ID 填入 gridea 客户端设置里即可</p>
<p><img src="/../../images/image-20230130172143421.png" alt="image-20230130172143421"></p>
<p><img src="/../../images/image-20230130172227849.png" alt="image-20230130172227849"></p>
</li>
<li><p>保存 gridea 设置并同步，等待几分钟保证设置生效。</p>
</li>
<li><p>接着可以回到网站列表，点击【代码检查】，百度会检查站点的统计 ID 是否已就绪。</p>
<p><img src="/../../images/image-20230130172315380.png" alt="image-20230130172315380"></p>
</li>
<li><p>搞定。</p>
</li>
</ol>
<h2 id="谷歌统计"><a href="#谷歌统计" class="headerlink" title="谷歌统计"></a>谷歌统计</h2><p>谷歌统计就有点难受了，主要是因为 gridea 只支持谷歌旧版的 Google Analytics。关键在于谷歌强推新版 GA4，并且将于 2023-07 停止收集 Google Analytics，看 gridea 作者能否尽快迭代跟进了。如果届时没有支持的话，也许可以试下在 gridea【基础配置-底部信息】直接贴谷歌的 js 代码，未验证。</p>
<ol>
<li><p>登录谷歌账号到谷歌统计，点击谷歌统计后台右下角的【管理】</p>
</li>
<li><p>首先我们需要申请一个谷歌统计账号，点击管理界面中的【创建账号】，创建账号主要分三步分别是账号开设、媒体资源设置、关于您的商家。【账号开设】看情况填</p>
<p><img src="/../../images/image-20230130173738614.png" alt="image-20230130173738614"></p>
</li>
<li><p>使用旧版 Google Analytics 的话，需要注意在【媒体资源设置】时，点击【显示高级选项】并打开【创建 Universal Analytics 媒体资源】。这里谷歌也有红字提醒了 20230701 停止服务的信息。下面选【同时创建 GA 和 GA4 媒体资源】（另一个也行）</p>
<p><img src="/../../images/image-20230130174043492.png" alt="image-20230130174043492"></p>
</li>
<li><p>【关于您的商家】随便填</p>
</li>
<li><p>然后就创建好了，谷歌会跳转到媒体资源详情页。这里没有我们需要的统计 ID，回到上一层，注意看左上角的媒体资源列表，展开其中带 UA 字样的就是我们目前需要的统计 ID（或【跟踪信息-跟踪代码】）</p>
<p><img src="/../../images/image-20230130174736444.png" alt="image-20230130174736444"></p>
</li>
<li><p>复制贴到 gridea 客户端里，保存并同步。</p>
</li>
<li><p>没注意到谷歌统计有没有验证 ID 正确性的地方，笔者是自己手动测试数据收集的哈哈。</p>
</li>
<li><p>搞定</p>
</li>
</ol>
<h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><p><a href="https://tongji.baidu.com/web5/10000523480/welcome/login">百度统计</a></p>
<p><a href="https://tongji.baidu.com/web/help/article?id=174&type=0">百度统计文档</a></p>
<p><a href="https://analytics.google.com/analytics/web">谷歌统计</a></p>
<p><a href="https://support.google.com/analytics/answer/6132368">谷歌统计文档</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/364820717">UA 和 GA4 的区别</a></p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>go 的 var 和 :=</title>
    <url>/2023/02/9a311670/</url>
    <content><![CDATA[<p>初学的时候看到 go 的变量声明还分两种 var 和 :&#x3D;，使用上各种混着用差点给我劝退，辛亏自己是个查（抄）资料的孩子。</p>
<h2 id="var"><a href="#var" class="headerlink" title="var"></a>var</h2><blockquote>
<p>纯正且符合印象的变量声明关键字</p>
</blockquote>
<h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> variable [<span class="keyword">type</span>] [= expression]</span><br></pre></td></tr></table></figure>



<p>type: 变量类型，如未声明则会进行类型推断</p>
<p>expression: 变量初始值</p>
<h3 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h3><ol>
<li>使用 var 时 type 和 expression 可选但不能同时省略，否则将得到致命错误，如 var apple</li>
<li>var 声明不指定 expression 时，go 会根据 type 自动给出默认值— go 中变量声明后必定会有初始值，只不过是隐式指定还是显式指定，如下：</li>
</ol>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"># /Users/hewenqian/apps/<span class="keyword">go</span>/go_study/test/var_test.<span class="keyword">go</span></span><br><span class="line"><span class="keyword">package</span> test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TestVar</span><span class="params">(t *testing.T)</span></span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> i <span class="type">int</span></span><br><span class="line">    <span class="keyword">var</span> s <span class="type">string</span></span><br><span class="line">    <span class="keyword">var</span> f <span class="type">float64</span></span><br><span class="line">    <span class="keyword">var</span> b <span class="type">bool</span></span><br><span class="line">    # 空对象</span><br><span class="line">    <span class="keyword">var</span> o <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    fmt.Printf(<span class="string">&quot;i is %d,\n&quot;</span>, i)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;s is %s,\n&quot;</span>, s)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;f is %f,\n&quot;</span>, f)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;b is %t,\n&quot;</span>, b)</span><br><span class="line">    <span class="comment">// 这里 %v 和 %+v 有啥区别</span></span><br><span class="line">    fmt.Printf(<span class="string">&quot;o is %v,\n&quot;</span>, o)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  go_study git:(main) go test -v ./test/var_test.go</span><br><span class="line">=== RUN   TestVar</span><br><span class="line">i is 0,</span><br><span class="line">s is ,</span><br><span class="line">f is 0.000000,</span><br><span class="line">b is false,</span><br><span class="line">o is &lt;nil&gt;,</span><br><span class="line">--- PASS: TestVar (0.00s)</span><br><span class="line">PASS</span><br><span class="line">ok      command-line-arguments    0.252s</span><br><span class="line">➜  go_study git:(main)</span><br></pre></td></tr></table></figure>



<p>综上，虽然定义语句没有显式指定变量值，但人家是有值的。</p>
<p>思考：有哪些语言的哪些类型可以是声明时未指定默认值（底层的话应该是值未分配具体内存空间？）的呢？</p>
<h2 id="x3D"><a href="#x3D" class="headerlink" title=":&#x3D;"></a>:&#x3D;</h2><p>:&#x3D; 主要用于函数内声明局部变量。</p>
<h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><p>variable :&#x3D; 表达式或值</p>
<p>变量的类型有 go 针对 [表达式或值自动] 推断</p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title>基于 github.io 博客的 gitbook 搭建</title>
    <url>/2023/02/ad836518/</url>
    <content><![CDATA[<p>尽管已经搭建了个人博客，依然准备额外搭建一个个人 gitbook。个人博客用于常规笔记和分享，gitbook 则学习、阅读侧重笔记，将线上课程笔记之类的成体系梳理，方便以后查阅。</p>
<h1 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash</span><br><span class="line">nvm install node</span><br><span class="line">nvm install npm</span><br><span class="line">npm install -g gitbook-cli</span><br><span class="line">gitbook -V</span><br></pre></td></tr></table></figure>



<h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><p>环境安装完成后，创建两个目录，gitbook 和 git_book_local</p>
<p>gitbook 是 github 仓库的本地目录，用作版本管理</p>
<p>gitbook_local 作为本地编辑 gitbook 的工作目录</p>
<p>markdown 编辑待发布内容并同步到 gitbook_local 目录下，然后执行初始化和构建等操作，操作命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 在新增新目录或初始化时执行</span><br><span class="line">gitbook init</span><br><span class="line"></span><br><span class="line"># 构建静态页面，我发现这个 build 的 --output= 选项没屁用，先不纠结了哈哈</span><br><span class="line">gitbook build</span><br><span class="line"></span><br><span class="line"># 启动本地预览服务器</span><br><span class="line">gitbook serve</span><br></pre></td></tr></table></figure>

<p>本地预览没问题后，将 build 构建的 _book 目录内容拷贝到 gitbook 目录下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 注意这个 _book 后面的 /</span><br><span class="line">cd ~/apps/gitbook_local</span><br><span class="line">cp -r ./_book/ ../gitbook/</span><br></pre></td></tr></table></figure>

<p>然后提交 gitbook 项目 到 github 仓库</p>
<p><img src="/../../images/gitbook%E9%A1%B9%E7%9B%AE.png" alt="gitbook项目"></p>
<p>项目发布后进入 github 项目的 settings -&gt; Pages 开启 GitHub Pages（仅第一次需要），如下</p>
<p>![github page开启](..&#x2F;..&#x2F;images&#x2F;github page开启.png)</p>
<p>最终的访问路径 <a href="https://hwqdqwh.github.io/gitbook/">https://hwqdqwh.github.io/gitbook/</a> 贼爽</p>
<p>最终效果：</p>
<p><img src="/../../images/%E6%95%88%E6%9E%9C%E5%9B%BE.png" alt="效果图"></p>
<h1 id="网页端-gitbook"><a href="#网页端-gitbook" class="headerlink" title="网页端 gitbook"></a>网页端 gitbook</h1><p>还试用了 gitbook 的网页端，<a href="https://app.gitbook.com/">https://app.gitbook.com/</a></p>
<p>优点：</p>
<p>不需要安装本地环境，注册后关联 github 即可使用，就是快又简单一些</p>
<p>缺点：</p>
<ul>
<li><p>gitbook 线上端发布后是另一个域名 <a href="https://hwqdqwh.gitbook.io/hwqdqwh/">https://hwqdqwh.gitbook.io/hwqdqwh/</a> 与目标不符，我本意还是放在 github pages 下跟博客背靠背</p>
</li>
<li><p>gitbook 线上端看了网上关于其版本的迭代，没有提供客户端的编辑工具，只能网站操作不喜欢</p>
</li>
<li><p>gitbook 是独立的版本管理（虽然可以同步到 github）</p>
</li>
<li><p>gitbook 发布后的页面主题非常不喜欢（它这居中布局略丑）</p>
</li>
</ul>
<p>![gitbook web效果图](..&#x2F;..&#x2F;images&#x2F;gitbook web效果图.png)</p>
<h1 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h1><h3 id="mac-下-markdown-编辑目录不是-gitbook-local-时的处理"><a href="#mac-下-markdown-编辑目录不是-gitbook-local-时的处理" class="headerlink" title="mac 下 markdown 编辑目录不是 gitbook_local 时的处理"></a>mac 下 markdown 编辑目录不是 gitbook_local 时的处理</h3><p>在创建软链时还遇到 No such file or directory 的奇怪问题</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ln –sf &quot;/Users/hewenqian/markdown/基于\ github.io\ 博客的\ gitbook\ 搭建.md&quot; /Users/hewenqian/apps/gitbook_local/gitbook/gitbook.md</span><br><span class="line">ln: /Users/hewenqian/apps/gitbook_local/gitbook/gitbook.md: No such file or directory</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>crontab 任务配置基础</title>
    <url>/2023/02/fadd76df/</url>
    <content><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><p>* * * * * command</p>
<p>五个占位依次对应分钟、小时、日、月、周几，这些很基础不需要太多整理，记住周几取值[0,6]，0 表示周日即可。</p>
<h1 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h1><h2 id="字段执行形式"><a href="#字段执行形式" class="headerlink" title="字段执行形式"></a>字段执行形式</h2><p>忽然有一天想到一个问题，*&#x2F;45 * * * * 会以怎样的形式执行呢？</p>
<p>首先肯定是每小时的 45 分都会执行，那么假设上一次执行是 00:45，下一次执行是 01:30 还是其它时间点？</p>
<p>动手测一下，crontab -e 添加任务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">*/7 * * * * date &gt;&gt; /work/tmp/test_crontab.log</span><br></pre></td></tr></table></figure>

<p>tail -100f &#x2F;work&#x2F;tmp&#x2F;test_crontab.log 静候可以发现，在跨小时时的执行时间为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Mon Jun 13 17:30:01 CST 2022 # 第一个是手动执行，为了获得当前时间</span><br><span class="line">Mon Jun 13 17:35:01 CST 2022</span><br><span class="line">Mon Jun 13 17:42:02 CST 2022</span><br><span class="line">Mon Jun 13 17:49:02 CST 2022</span><br><span class="line">Mon Jun 13 17:56:01 CST 2022</span><br><span class="line">Mon Jun 13 18:00:01 CST 2022</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<p>由此可见：</p>
<ol>
<li>*&#x2F;7 不是从 crontab -e 创建任务开始算的，而是每小时的 0 分开始算，比如在 00:30 创建了上述任务，下次执行是 00:35 而不是 00:37</li>
<li>每个小时的间隔是单独计算的。所以 17:56 执行获得输出后，下一次执行是 18:00 而不是 18:03</li>
<li>*&#x2F;7 意思是【每小时从 0 分起，每隔 7 分钟执行一次】，所以第一次执行是 0 分，其它诸如 <em>&#x2F;20、</em>&#x2F;45 同理，如果仅希望每小时第 45 分执行一次任务，则应该配置为 45 * * * *</li>
</ol>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>组织一个可维护可扩展的 go 项目目录</title>
    <url>/2023/02/df63fc18/</url>
    <content><![CDATA[<blockquote>
<p>摘自极客课程，限时学习的课程是真的令人无语</p>
</blockquote>
<h1 id="如何规范目录"><a href="#如何规范目录" class="headerlink" title="如何规范目录"></a>如何规范目录</h1><p>一个好的目录结构至少需要满足以下原则：</p>
<ul>
<li><p>命名清晰：目录命名要清晰、简洁，不要太长，也不要太短，目录名要能清晰地表达出该目录实现的功能，并且目录名最好用单数。一方面是因为单数足以说明这个目录的功能，另一方面可以统一规范，避免单复混用的情况。</p>
</li>
<li><p>功能明确：一个目录所要实现的功能应该是明确的、并且在整个项目目录中具有很高的辨识度。也就是说，当需要新增一个功能时，我们能够非常清楚地知道把这个功能放在哪个目录下。</p>
</li>
<li><p>全面性：目录结构应该尽可能全面地包含研发过程中需要的功能，例如文档、脚本、源码管理、API 实现、工具、第三方包、测试、编译产物等。</p>
</li>
<li><p>可观测性：项目规模一定是从小到大的，所以一个好的目录结构应该能够在项目变大时，仍然保持之前的目录结构。</p>
</li>
<li><p>可扩展性：每个目录下存放了同类的功能，在项目变大时，这些目录应该可以存放更多同类功能。</p>
</li>
</ul>
<p>根据功能，我们可以将目录结构分为结构化目录结构和平铺式目录结构。结构化目录结构主要用在 Go 应用中，平铺式目录结构主要用在 Go 包中。</p>
<h1 id="结构化目录结构"><a href="#结构化目录结构" class="headerlink" title="结构化目录结构"></a>结构化目录结构</h1><p>当前 Go 社区比较推荐的结构化目录结构是 project-layout。形式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── api</span><br><span class="line">│   ├── openapi</span><br><span class="line">│   └── swagger</span><br><span class="line">├── build</span><br><span class="line">│   ├── ci</span><br><span class="line">│   ├── docker</span><br><span class="line">│   │   ├── iam-apiserver</span><br><span class="line">│   │   ├── iam-authz-server</span><br><span class="line">│   │   └── iam-pump</span><br><span class="line">│   ├── package</span><br><span class="line">├── CHANGELOG</span><br><span class="line">├── cmd</span><br><span class="line">│   ├── iam-apiserver</span><br><span class="line">│   │   └── apiserver.go</span><br><span class="line">│   ├── iam-authz-server</span><br><span class="line">│   │   └── authzserver.go</span><br><span class="line">│   ├── iamctl</span><br><span class="line">│   │   └── iamctl.go</span><br><span class="line">│   └── iam-pump</span><br><span class="line">│       └── pump.go</span><br><span class="line">├── configs</span><br><span class="line">├── CONTRIBUTING.md</span><br><span class="line">├── deployments</span><br><span class="line">├── docs</span><br><span class="line">│   ├── devel</span><br><span class="line">│   │   ├── en-US</span><br><span class="line">│   │   └── zh-CN</span><br><span class="line">│   ├── guide</span><br><span class="line">│   │   ├── en-US</span><br><span class="line">│   │   └── zh-CN</span><br><span class="line">│   ├── images</span><br><span class="line">│   └── README.md</span><br><span class="line">├── examples</span><br><span class="line">├── githooks</span><br><span class="line">├── go.mod</span><br><span class="line">├── go.sum</span><br><span class="line">├── init</span><br><span class="line">├── internal</span><br><span class="line">│   ├── apiserver</span><br><span class="line">│   │   ├── api</span><br><span class="line">│   │   │   └── v1</span><br><span class="line">│   │   │       └── user</span><br><span class="line">│   │   ├── apiserver.go</span><br><span class="line">│   │   ├── options</span><br><span class="line">│   │   ├── service</span><br><span class="line">│   │   ├── store</span><br><span class="line">│   │   │   ├── mysql</span><br><span class="line">│   │   │   ├── fake</span><br><span class="line">│   │   └── testing</span><br><span class="line">│   ├── authzserver</span><br><span class="line">│   │   ├── api</span><br><span class="line">│   │   │   └── v1</span><br><span class="line">│   │   │       └── authorize</span><br><span class="line">│   │   ├── options</span><br><span class="line">│   │   ├── store</span><br><span class="line">│   │   └── testing</span><br><span class="line">│   ├── iamctl</span><br><span class="line">│   │   ├── cmd</span><br><span class="line">│   │   │   ├── completion</span><br><span class="line">│   │   │   ├── user</span><br><span class="line">│   │   └── util</span><br><span class="line">│   ├── pkg</span><br><span class="line">│   │   ├── code</span><br><span class="line">│   │   ├── options</span><br><span class="line">│   │   ├── server</span><br><span class="line">│   │   ├── util</span><br><span class="line">│   │   └── validation</span><br><span class="line">├── LICENSE</span><br><span class="line">├── Makefile</span><br><span class="line">├── _output</span><br><span class="line">│   ├── platforms</span><br><span class="line">│   │   └── linux</span><br><span class="line">│   │       └── amd64</span><br><span class="line">├── pkg</span><br><span class="line">│   ├── util</span><br><span class="line">│   │   └── genutil</span><br><span class="line">├── README.md</span><br><span class="line">├── scripts</span><br><span class="line">│   ├── lib</span><br><span class="line">│   ├── make-rules</span><br><span class="line">├── test</span><br><span class="line">│   ├── testdata</span><br><span class="line">├── third_party</span><br><span class="line">│   └── forked</span><br><span class="line">└── tools</span><br></pre></td></tr></table></figure>

<p>结合项目开发和管理流程，大致上可以按照以下方式约定。</p>
<p><img src="/../../images/image-20230202142307743.png" alt="image-20230202142307743"></p>
<p>解释：</p>
<h2 id="web"><a href="#web" class="headerlink" title="web"></a>web</h2><p>前端代码存放目录，主要用来存放 Web 静态资源，服务端模板和单页应用（SPAs）。</p>
<h2 id="cmd"><a href="#cmd" class="headerlink" title="cmd"></a>cmd</h2><p>一个项目有很多组件，可以把组件 main 函数所在的文件夹统一放在&#x2F;cmd 目录下</p>
<h2 id="internal"><a href="#internal" class="headerlink" title="internal"></a>internal</h2><p>存放私有应用和库代码。如果一些代码，你不希望在其他应用和库中被导入，可以将这部分代码放在&#x2F;internal 目录下。</p>
<blockquote>
<p>在引入其它项目 internal 下的包时，Go 语言会在编译时报错：</p>
</blockquote>
<p>&#x2F;internal&#x2F;apiserver：该目录中存放真实的应用代码。这些应用的共享代码存放在&#x2F;internal&#x2F;pkg 目录下。</p>
<p>&#x2F;internal&#x2F;pkg：存放项目内可共享，项目外不共享的包。这些包提供了比较基础、通用的功能，例如工具、错误码、用户验证等功能。</p>
<p>建议是，一开始将所有的共享代码存放在 &#x2F;internal&#x2F;pkg 目录下，当该共享代码做好了对外开发的准备后，再转存到&#x2F;pkg目录下。</p>
<p>&#x2F;internal 目录大概分为 3 类子目录：</p>
<ul>
<li>&#x2F;internal&#x2F;pkg：内部共享包存放的目录。</li>
<li>&#x2F;internal&#x2F;authzserver、&#x2F;internal&#x2F;apiserver、&#x2F;internal&#x2F;pump、&#x2F;internal&#x2F;iamctl：应用目录，里面包含应用程序的实现代码。</li>
<li>&#x2F;internal&#x2F;iamctl：对于一些大型项目，可能还会需要一个客户端工具。</li>
</ul>
<p>在每个应用程序内部，也会有一些目录结构，这些目录结构主要根据功能来划分：</p>
<ul>
<li>&#x2F;internal&#x2F;apiserver&#x2F;api&#x2F;v1：HTTP API 接口的具体实现，主要用来做 HTTP 请求的解包、参数校验、业务逻辑处理、返回。注意这里的业务逻辑处理应该是轻量级的，如果业务逻辑比较复杂，代码量比较多，建议放到 &#x2F;internal&#x2F;apiserver&#x2F;service 目录下。该源码文件主要用来串流程。</li>
<li>&#x2F;internal&#x2F;apiserver&#x2F;options：应用的 command flag。</li>
<li>&#x2F;internal&#x2F;apiserver&#x2F;config：根据命令行参数创建应用配置。</li>
<li>&#x2F;internal&#x2F;apiserver&#x2F;service：存放应用复杂业务处理代码。</li>
<li>&#x2F;internal&#x2F;apiserver&#x2F;store&#x2F;mysql：一个应用可能要持久化的存储一些数据，这里主要存放跟数据库交互的代码，比如 Create、Update、Delete、Get、List 等。</li>
</ul>
<p>&#x2F;internal&#x2F;pkg 目录存放项目内可共享的包，通常可以包含如下目录：</p>
<ul>
<li>&#x2F;internal&#x2F;pkg&#x2F;code：项目业务 Code 码。</li>
<li>&#x2F;internal&#x2F;pkg&#x2F;validation：一些通用的验证函数。</li>
<li>&#x2F;internal&#x2F;pkg&#x2F;middleware：HTTP 处理链。</li>
</ul>
<h2 id="pkg"><a href="#pkg" class="headerlink" title="pkg"></a>pkg</h2><p>该目录中存放可以被外部应用使用的代码库，其他项目可以直接通过 import 导入这里的代码。</p>
<h2 id="vendor"><a href="#vendor" class="headerlink" title="vendor"></a>vendor</h2><p>项目依赖，可通过 go mod vendor 创建。需要注意的是，如果是一个 Go 库，不要提交 vendor 依赖包。</p>
<h2 id="third-party"><a href="#third-party" class="headerlink" title="third_party"></a>third_party</h2><p>外部帮助工具，分支代码或其他第三方应用（例如 Swagger UI）。比如我们 fork 了一个第三方 go 包，并做了一些小的改动，我们可以放在目录 &#x2F;third_party&#x2F;forked 下。一方面可以很清楚的知道该包是 fork 第三方的，另一方面又能够方便地和 upstream 同步。</p>
<h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><p>用于存放其他外部测试应用和测试数据。</p>
<h2 id="configs"><a href="#configs" class="headerlink" title="configs"></a>configs</h2><p>这个目录用来配置文件模板或默认配置。有一点要注意，配置中不能携带敏感信息，这些敏感信息，我们可以用占位符来替代。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1    </span><br><span class="line">user:    </span><br><span class="line">  username: $&#123;CONFIG_USER_USERNAME&#125; # iam 用户名    </span><br><span class="line">  password: $&#123;CONFIG_USER_PASSWORD&#125; # iam 密码</span><br></pre></td></tr></table></figure>

<h2 id="deployments"><a href="#deployments" class="headerlink" title="deployments"></a>deployments</h2><p>用来存放 Iaas、PaaS 系统和容器编排部署配置和模板（Docker-Compose，Kubernetes&#x2F;Helm，Mesos，Terraform，Bosh）。在一些项目，特别是用 Kubernetes 部署的项目中，这个目录可能命名为 deploy。</p>
<h2 id="init"><a href="#init" class="headerlink" title="init"></a>init</h2><p>存放初始化系统（systemd，upstart，sysv）和进程管理配置文件（runit，supervisord）</p>
<h2 id="Makefile"><a href="#Makefile" class="headerlink" title="Makefile"></a>Makefile</h2><p>一个 Go 项目在其根目录下应该有一个 Makefile 工具，用来对项目进行管理，Makefile 通常用来执行静态代码检查、单元测试、编译等功能。</p>
<h2 id="scripts"><a href="#scripts" class="headerlink" title="scripts"></a>scripts</h2><p>该目录主要用来存放脚本文件，实现构建、安装、分析等不同功能。不同项目，里面可能存放不同的文件，但通常可以考虑包含以下 3 个目录：</p>
<ul>
<li>&#x2F;scripts&#x2F;make-rules：用来存放 makefile 文件，实现 &#x2F;Makefile 文件中的各个功能。Makefile 有很多功能，为了保持它的简洁，我建议你将各个功能的具体实现放在&#x2F;scripts&#x2F;make-rules 文件夹下。</li>
<li>&#x2F;scripts&#x2F;lib：shell 库，用来存放 shell 脚本。一个大型项目中有很多自动化任务，比如发布、更新文档、生成代码等，所以要写很多 shell 脚本，这些 shell 脚本会有一些通用功能，可以抽象成库，存放在&#x2F;scripts&#x2F;lib 目录下，比如 logging.sh，util.sh 等。</li>
<li>&#x2F;scripts&#x2F;install：如果项目支持自动化部署，可以将自动化部署脚本放在此目录下。如果部署脚本简单，也可以直接放在 &#x2F;scripts 目录下。</li>
</ul>
<h2 id="build"><a href="#build" class="headerlink" title="build"></a>build</h2><p>这里存放安装包和持续集成相关的文件。这个目录下有 3 个大概率会使用到的目录，在设计目录结构时可以考虑进去。</p>
<ul>
<li>&#x2F;build&#x2F;package：存放容器（Docker）、系统（deb, rpm, pkg）的包配置和脚本。</li>
<li>&#x2F;build&#x2F;ci：存放 CI（travis，circle，drone）的配置文件和脚本。</li>
<li>&#x2F;build&#x2F;docker：存放子项目各个组件的 Dockerfile 文件。</li>
</ul>
<h2 id="tools"><a href="#tools" class="headerlink" title="tools"></a>tools</h2><p>存放这个项目的支持工具。这些工具可导入来自 &#x2F;pkg 和 &#x2F;internal 目录的代码。</p>
<h2 id="githooks"><a href="#githooks" class="headerlink" title="githooks"></a>githooks</h2><p>Git 钩子。比如，我们可以将 commit-msg 存放在该目录。</p>
<h2 id="assets"><a href="#assets" class="headerlink" title="assets"></a>assets</h2><p>项目使用的其他资源 (图片、CSS、JavaScript 等)。</p>
<h2 id="website"><a href="#website" class="headerlink" title="website"></a>website</h2><p>如果你不使用 GitHub 页面，那么可以在这里放置项目网站相关的数据。</p>
<h2 id="README-md"><a href="#README-md" class="headerlink" title="README.md"></a>README.md</h2><p>项目的 README 文件一般包含了项目的介绍、功能、快速安装和使用指引、详细的文档链接以及开发指引等。</p>
<h2 id="docs"><a href="#docs" class="headerlink" title="docs"></a>docs</h2><p>存放设计文档、开发文档和用户文档等（除了 godoc 生成的文档）</p>
<h2 id="CONTRIBUTING-md"><a href="#CONTRIBUTING-md" class="headerlink" title="CONTRIBUTING.md"></a>CONTRIBUTING.md</h2><p>如果是一个开源就绪的项目，最好还要有一个 CONTRIBUTING.md 文件，用来说明如何贡献代码，如何开源协同等等。CONTRIBUTING.md 不仅能够规范协同流程，还能降低第三方开发者贡献代码的难度。</p>
<h2 id="api"><a href="#api" class="headerlink" title="api"></a>api</h2><p>api 目录中存放的是当前项目对外提供的各种不同类型的 API 接口定义文件，其中可能包含类似 &#x2F;api&#x2F;protobuf-spec、&#x2F;api&#x2F;thrift-spec、&#x2F;api&#x2F;http-spec、openapi、swagger 的目录，这些目录包含了当前项目对外提供和依赖的所有 API 文件。</p>
<h2 id="LICENSE"><a href="#LICENSE" class="headerlink" title="LICENSE"></a>LICENSE</h2><p>版权文件可以是私有的，也可以是开源的。常用的开源协议有：Apache 2.0、MIT、BSD、GPL、Mozilla、LGPL。</p>
<p>有时候，公有云产品为了打造品牌影响力，会对外发布一个本产品的开源版本，所以在项目规划初期最好就能规划下未来产品的走向，选择合适的 LICENSE。为了声明版权，你可能会需要将 LICENSE 头添加到源码文件或者其他文件中，这部分工作可以通过工具实现自动化，推荐工具： addlicense 。</p>
<p>当代码中引用了其它开源代码时，需要在 LICENSE 中说明对其它源码的引用，这就需要知道代码引用了哪些源码，以及这些源码的开源协议，可以借助工具来进行检查，推荐工具： glice 。至于如何说明对其它源码的引用，大家可以参考下 IAM 项目的 LICENSE 文件。</p>
<h2 id="CHANGELOG"><a href="#CHANGELOG" class="headerlink" title="CHANGELOG"></a>CHANGELOG</h2><p>当项目有更新时，为了方便了解当前版本的更新内容或者历史更新内容，需要将更新记录存放到 CHANGELOG 目录。</p>
<h2 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h2><p>存放应用程序或者公共包的示例代码。这些示例代码可以降低使用者的上手门槛。</p>
<h1 id="平铺式目录结构"><a href="#平铺式目录结构" class="headerlink" title="平铺式目录结构"></a>平铺式目录结构</h1><p>平铺式就是在项目的根目录下直接存放项目代码，整儿个目录结构看起来像是一层的大平房。好处是引用路径长度明显减少，相应地要求项目专注实现项目规划本身的内容，不再像业务项目一样海纳百川了。</p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 常用命令安装</title>
    <url>/2023/02/84201577/</url>
    <content><![CDATA[<p>-bash: rz: 未找到命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install lrzsz</span><br></pre></td></tr></table></figure>

<p>-bash: unzip: 未找到命令</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install zip</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Anaconda 使用</title>
    <url>/2023/02/f053b2c8/</url>
    <content><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Anaconda 是一个免费开源的 Python  和 R 发行版本，主要用于解决机器学习和数据科学应用中的环境管理和部署难题。在对接量化数据做分析的时候用到，用起来还蛮方便的，记录一下。</p>
<p>miniconda 是 Anaconda 的最小化干净安装版本，仅包含基础的 conda、Python、基础的依赖包和 zlib 等常用包。喜欢服务器干净整洁的同学可以选择 miniconda。</p>
<span id="more"></span>

<p>Anaconda 中的 conda 则是一个开源跨平台的包管理工具和系统管理工具。关于 conda 包管理和系统管理得分开理解。</p>
<p>conda 作为包管理工具时一如 pip 之于 Python，用来管理环境包的安装。在 Anaconda 中的 pip 的作用可以视作 conda 的子集。</p>
<p>作为系统管理工具时，conda 类似 docker 拿来即用的思想，支持快速构建独立虚拟环境，实现进程级别的隔离。不过需要注意 conda 和 docker 的区别：虚拟环境和容器的区别。conda 和 venv、virtualenv 一样通过环境变量（如 $PATH）替换实现环境隔离，虚拟环境安装的工具（如 Python3.8）宿主机也能找到其解释器。所以虚拟环境可以在不激活环境的情况下，通过手动指定完整路径执行不同版本的解释器。而 docker 的容器环境隔离依托 linux 系统的 namespace 技术实现，容器安装的特定版本语言解释器在宿主机是没有的。</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>首先到 Anaconda 官网找到自己想下载的版本，wget 下载并执行之</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.3.1-Linux-x86_64.sh</span><br><span class="line">bash Anaconda3-5.3.1-Linux-x86_64.sh -u</span><br></pre></td></tr></table></figure>

<p>enter 确认，然后是用户协议，输入 yes</p>
<p>然后是指定安装路径</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Anaconda3 will now be installed into this location:</span><br><span class="line">/root/anaconda3</span><br><span class="line">  - Press ENTER to confirm the location</span><br><span class="line">  - Press CTRL-C to abort the installation</span><br><span class="line">  - Or specify a different location below</span><br></pre></td></tr></table></figure>

<p>安装完成后可以选择是否在环境参数中添加 Anaconda 参数，yes</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">installation finished.</span><br><span class="line">Do you wish the installer to initialize Anaconda3</span><br><span class="line">in your /root/.bashrc ? [yes|no]</span><br><span class="line"></span><br><span class="line">[no] &gt;&gt;&gt; yes</span><br></pre></td></tr></table></figure>

<p>是否安装 VSCode，no</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Anaconda is partnered with Microsoft! Microsoft VSCode is a streamlined</span><br><span class="line">code editor with support for development operations like debugging, task</span><br><span class="line">running and version control.</span><br><span class="line"></span><br><span class="line">To install Visual Studio Code, you will need:</span><br><span class="line">  - Administrator Privileges</span><br><span class="line">  - Internet connectivity</span><br><span class="line"></span><br><span class="line">Visual Studio Code License: https://code.visualstudio.com/license</span><br><span class="line">Do you wish to proceed with the installation of Microsoft VSCode? [yes|no]</span><br><span class="line">&gt;&gt;&gt; no</span><br></pre></td></tr></table></figure>

<p>完成，验证一下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 刷新环境变量</span><br><span class="line">source .bash_profile</span><br><span class="line"></span><br><span class="line"># 校验</span><br><span class="line">conda</span><br><span class="line"></span><br><span class="line"># 看到诸如以下的 help 指南就是成功了</span><br><span class="line">usage: conda [-h] [-V] command ...</span><br><span class="line">conda is a tool for managing and deploying applications, environments and packages.</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>使用命令看一下 <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/environments.html">conda文档</a> 就好😘</p>
<h2 id="踩坑记录"><a href="#踩坑记录" class="headerlink" title="踩坑记录"></a>踩坑记录</h2><h3 id="No-module-named-conda"><a href="#No-module-named-conda" class="headerlink" title="No module named conda"></a>No module named conda</h3><p>最后记录一下踩的坑</p>
<p>修改环境的 Python 版本时， 有一次执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install python=3.8</span><br></pre></td></tr></table></figure>

<p>得到了奇怪的报错：ModuleNotFoundError: No module named ‘conda’ 可有点秀，conda 报错没有名为 conda 的模块吗哈哈</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Preparing transaction: done</span><br><span class="line">Verifying transaction: done</span><br><span class="line">Executing transaction: done</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">	File &quot;/root/anaconda3/bin/conda&quot;, line 7, in &lt;module&gt;</span><br><span class="line">		from conda.cli import main</span><br><span class="line">		</span><br><span class="line">ModuleNotFoundError: No module named &#x27;conda&#x27;</span><br></pre></td></tr></table></figure>

<p>其实这类问题很普遍，几乎都是环境变量配置问题，常见于修改 base 环境的 Python 版本导致解释器找不到当前 conda 包。所以不要随便修改 base 环境 Python 版本，可以多开个虚拟环境嘛</p>
<p>解决的话只要找回之前安装 Anaconda 的脚本执行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bash Anaconda3-5.1.0-Linux-x86_64.sh -u</span><br></pre></td></tr></table></figure>

<p>即可</p>
<p>附，想要更新 base 环境的 Python 版本的话可以参考 <a href="https://www.cnblogs.com/ruhai/p/12684838.html">升级Anaconda默认的python版本</a></p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Python __init__ 和 __new__</title>
    <url>/2023/02/b1009463/</url>
    <content><![CDATA[<p>__init__ 是初始化方法，没有返回值</p>
<p>__new__ 才是真正的构造方法，返回当前类实例化对象</p>
<span id="more"></span>

<h1 id="执行顺序"><a href="#执行顺序" class="headerlink" title="执行顺序"></a>执行顺序</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">testOrder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;this is init&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__new__</span>(<span class="params">cls, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;this is new&#x27;</span>)</span><br><span class="line">        <span class="comment"># return object() 返回 object 类实例对象</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().__new__(cls)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">beCalled</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;called by u&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    test = testOrder()</span><br><span class="line">    test.beCalled()</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜ python python3 study/init_new.py</span><br><span class="line">this is new</span><br><span class="line">this is init</span><br><span class="line">called by u</span><br><span class="line">➜ python</span><br></pre></td></tr></table></figure>

<p>输出表明了 __init__，__new__ 和 常规函数的调用顺序是 new &gt; init &gt; others</p>
<h1 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h1><p>__init__ 中的第一个参数 self 代表对象自身；__new__ 的 cls 则代表当前类本身，此方法必须返回一个类实例对象（可以是当前类或其它类）。</p>
<p>需要注意的是：解释器是依据 __new__ 返回的对象调用 __init__ 的，所以如果 __new__ 中返回了其它类的实例对象，将调用其它类的 __init__ 而不是当前类的 __init__ 。</p>
<p>例如将上述代码的 return super().__new__(cls) 修改为 return object() 后，仅会得到 this is new 的输出</p>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://docs.python.org/3/reference/datamodel.html?highlight=__new__#object.__new__">Python3 文档对 __new__ 的解释</a></p>
<p><a href="https://peps.python.org/pep-3114/">PEP 3114 new 重命名为 __new__</a></p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>pstree/strace/traceroute 和 lsof</title>
    <url>/2023/02/92e8b7bd/</url>
    <content><![CDATA[<p>记录几个常用的 linux 命令</p>
<ul>
<li>pstree: 类似 ps 命令的进程查看工具，可以将进程以树形结构可视化输出。部分内核或挂载信息会被以 ? 代替</li>
<li>strace: 集诊断、调试、统计命令&#x2F;进程信息于一体的工具,注意它不是专业的调试工具，如 gdb</li>
<li>traceroute: 跟踪当前到目的主机数据报传输链路的命令</li>
<li>lsof: 基于一切皆文件的系统文件描述符查看命令</li>
</ul>
<span id="more"></span>

<h1 id="pstree"><a href="#pstree" class="headerlink" title="pstree"></a>pstree</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># mac</span><br><span class="line">brew install pstree</span><br><span class="line"></span><br><span class="line">#Fedora/Red Hat/CentOS</span><br><span class="line">yum -y install psmisc</span><br><span class="line"></span><br><span class="line">#Ubuntu/Debian</span><br><span class="line">apt-get install psmisc</span><br></pre></td></tr></table></figure>



<h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Usage: pstree [ -a ] [ -c ] [ -h | -H PID ] [ -l ] [ -n ] [ -p ] [ -g ] [ -u ]</span><br><span class="line">             [ -A | -G | -U ] [ PID | USER ]</span><br><span class="line"></span><br><span class="line">pstree -V</span><br><span class="line">Display a tree of processes.</span><br><span class="line"></span><br><span class="line">-a, --arguments     show command line arguments</span><br><span class="line">                    显示进程的完整指令及参数</span><br><span class="line">-A, --ascii               use ASCII line drawing characters</span><br><span class="line">-c, --compact         don&#x27;t compact identical subtrees</span><br><span class="line">-h, --highlight-all   highlight current process and its ancestors</span><br><span class="line">                       突出显示当前进程及其祖先,实测发现好像 -h 没生效,不知道是不是自己 iterm 设置的配色原因</span><br><span class="line">-H PID, --highlight-pid=PID highlight this process and its ancestors</span><br><span class="line">-g, --show-pgids    show process group ids; implies -c</span><br><span class="line">-G, --vt100           use VT100 line drawing characters</span><br><span class="line">-l, --long              don&#x27;t truncate long lines</span><br><span class="line">                         超长指令不再用 ... 代替</span><br><span class="line">-n, --numeric-sort  sort output by PID</span><br><span class="line">                           按 PID 排序显示,默认是按名称,似乎在centeros 中 nh 同时使用 h 不会生效</span><br><span class="line">-N type,--ns-sort=type        sort by namespace type (ipc, mnt, net, pid, user, uts)</span><br><span class="line">-p, --show-pids       show PIDs; implies -c</span><br><span class="line">                          显示 PID</span><br><span class="line">-s, --show-parents    show parents of the selected process</span><br><span class="line">                          显示父级进程</span><br><span class="line">-S, --ns-changes      show namespace transitions</span><br><span class="line">-u, --uid-changes     show uid transitions</span><br><span class="line">-U, --unicode         use UTF-8 (Unicode) line drawing characters</span><br><span class="line">-V, --version         display version information</span><br><span class="line">-Z,--security-context   show SELinux security contexts</span><br><span class="line"></span><br><span class="line">PID    start at this PID; default is 1 (init)</span><br><span class="line">USER   show only trees rooted at processes of this user</span><br></pre></td></tr></table></figure>



<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 查看全部进程的树形</span><br><span class="line">pstree -anp</span><br><span class="line"></span><br><span class="line"># 根据进程号查看所在树的信息</span><br><span class="line">pstree -anps pid</span><br></pre></td></tr></table></figure>



<h1 id="strace"><a href="#strace" class="headerlink" title="strace"></a>strace</h1><h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install strace</span><br></pre></td></tr></table></figure>



<h3 id="文档-1"><a href="#文档-1" class="headerlink" title="文档"></a>文档</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">usage: strace [-CdffhiqrtttTvVwxxy] [-I n] [-e expr]...</span><br><span class="line">             [-a column] [-o file] [-s strsize] [-P path]...</span><br><span class="line">             -p pid... / [-D] [-E var=val]... [-u username] PROG [ARGS]</span><br><span class="line">   or: strace -c[dfw] [-I n] [-e expr]... [-O overhead] [-S sortby]</span><br><span class="line">             -p pid... / [-D] [-E var=val]... [-u username] PROG [ARGS]</span><br><span class="line"></span><br><span class="line">Output format:</span><br><span class="line">  -a column      alignment COLUMN for printing syscall results (default 40)</span><br><span class="line">                    将系统调用结果根据字段对齐,默认 为40.</span><br><span class="line">  -i             print instruction pointer at time of syscall</span><br><span class="line">  -k             obtain stack trace between each syscall</span><br><span class="line">  -o file        send trace output to FILE instead of stderr                            </span><br><span class="line">  -q             suppress messages about attaching, detaching, etc. </span><br><span class="line">  -r             print relative timestamp</span><br><span class="line">               印出相对时间关于,,每一个系统调用.</span><br><span class="line">  -s strsize     limit length of print strings to STRSIZE chars (default 32)</span><br><span class="line">               指定输出的字符串的最大长度.默认为32.文件名一直全部输出.(超出长度用...代替)</span><br><span class="line">  -t             print absolute timestamp</span><br><span class="line">               在输出中的每一行前加上时间信息.不可跟 -c 混用, -t 和 -tt 是在行首记录当前时间</span><br><span class="line">  -tt            print absolute timestamp with usecs</span><br><span class="line">               在输出中的每一行前加上时间信息,微秒级.</span><br><span class="line">     -T             print time spent in each syscall</span><br><span class="line">                   每次调用最后显示消耗时间</span><br><span class="line">  -x             print non-ascii strings in hex</span><br><span class="line">               以十六进制打印非 ascii 字符串</span><br><span class="line">  -xx            print all strings in hex</span><br><span class="line">               所有字符串以十六进制形式输出.</span><br><span class="line">  -X format      set the format for printing of named constants and flags</span><br><span class="line">  -y             print paths associated with file descriptor arguments</span><br><span class="line">  -yy            print protocol specific information associated with socket file descriptors</span><br><span class="line"></span><br><span class="line">Statistics:</span><br><span class="line">  -c            count time, calls, and errors for each syscall and report summary </span><br><span class="line">               统计每一系统调用的所执行的时间,次数和出错的次数等.</span><br><span class="line">  -C             like -c but also print regular output</span><br><span class="line">  -O overhead    set overhead for tracing syscalls to OVERHEAD usecs</span><br><span class="line">  -S sortby      sort syscall counts by: time, calls, name, nothing (default time)</span><br><span class="line">               针对性排序</span><br><span class="line">  -w             summarise syscall latency (default is system time)</span><br><span class="line"></span><br><span class="line">Filtering:</span><br><span class="line">  -e expr        a qualifying expression: option=[!]all or option=[!]val1[,val2]...</span><br><span class="line">    options:    trace, abbrev, verbose, raw, signal, read, write, fault, inject, kvm</span><br><span class="line">  -P path        trace accesses to path                                                     </span><br><span class="line"></span><br><span class="line">Tracing:</span><br><span class="line">  -b execve      detach on execve syscall</span><br><span class="line">  -D             run tracer process as a detached grandchild, not as parent</span><br><span class="line">  -f             follow forks</span><br><span class="line">               跟踪由fork调用所产生的子进程.</span><br><span class="line">  -ff            follow forks with output into separate files</span><br><span class="line">               如果提供-o filename,则所有进程的跟踪结果输出到相应的filename.pid中,pid是各进程的进程号.</span><br><span class="line">  -I interruptible</span><br><span class="line"></span><br><span class="line">    1:          no signals are blocked</span><br><span class="line">    2:          fatal signals are blocked while decoding syscall (default)</span><br><span class="line">    3:          fatal signals are always blocked (default if &#x27;-o FILE PROG&#x27;)</span><br><span class="line">    4:          fatal signals and SIGTSTP (^Z) are always blocked</span><br><span class="line">                (useful to make &#x27;strace -o FILE PROG&#x27; not stop on ^Z)</span><br><span class="line"></span><br><span class="line">Startup:</span><br><span class="line"></span><br><span class="line">  -E var         remove var from the environment for command</span><br><span class="line">  -E var=val     put var=val in the environment for command</span><br><span class="line">  -p pid         trace process with process id PID, may be repeated</span><br><span class="line">               跟踪指定的进程pid.</span><br><span class="line">  -u username    run command as username handling setuid and/or setgid</span><br><span class="line">               以username的UID和GID执行被跟踪的命令</span><br><span class="line"></span><br><span class="line">Miscellaneous:</span><br><span class="line"></span><br><span class="line">  -d             enable debug output to stderr</span><br><span class="line">               输出strace关于标准错误的调试信息.,这个输出现在还不会看</span><br><span class="line">  -v             verbose mode: print unabbreviated argv, stat, termios, etc. args  </span><br><span class="line">  -h             print help message</span><br><span class="line">  -V             print version                  </span><br></pre></td></tr></table></figure>



<p>-e expr 指定一个表达式,用来控制如何跟踪.格式：[qualifier&#x3D;][!]value1[,value2]…</p>
<p>qualifier只能是 trace,abbrev,verbose,raw,signal,read,write其中之一.value是用来限定的符号或数字.默认的 qualifier是 trace.感叹号是否定符号.例如:-eopen等价于 -e trace&#x3D;open,表示只跟踪open调用.而-etrace!&#x3D;open 表示跟踪除了open以外的其他调用.有两个特殊的符号 all 和 none. 注意有些shell使用!来执行历史记录里的命令,所以要使用\</p>
<p>-e trace&#x3D;set 只跟踪指定的系统 调用.例如:-e trace&#x3D;open,close,rean,write表示只跟踪这四个系统调用.默认的为set&#x3D;all.</p>
<p>-e trace&#x3D;file 只跟踪有关文件操作的系统调用.</p>
<p>-e trace&#x3D;process 只跟踪有关进程控制的系统调用.</p>
<p>-e trace&#x3D;network 跟踪与网络有关的所有系统调用.</p>
<p>-e strace&#x3D;signal 跟踪所有与系统信号有关的 系统调用</p>
<p>-e trace&#x3D;ipc 跟踪所有与进程通讯有关的系统调用</p>
<p>-e abbrev&#x3D;set 设定strace输出的系统调用的结果集.-v 等与 abbrev&#x3D;none.默认为abbrev&#x3D;all.</p>
<p>-e raw&#x3D;set 将指定的系统调用的参数以十六进制显示.</p>
<p>-e signal&#x3D;set 指定跟踪的系统信号.默认为all.如 signal&#x3D;!SIGIO(或者signal&#x3D;!io),表示不跟踪SIGIO信号.</p>
<p>-e read&#x3D;set 输出从指定文件中读出 的数据.例如: -e read&#x3D;3,5</p>
<p>-e write&#x3D;set 输出写入到指定文件中的数据.</p>
<h2 id="常用命令-1"><a href="#常用命令-1" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># strace -c date 统计命令各个系统调用的耗时,包含空格的命令不需要引号标记</span><br><span class="line"></span><br><span class="line">2021年 12月 08日 星期三 18:23:50 CST</span><br><span class="line">% time     seconds  usecs/call     calls    errors syscall</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line"> 61.33    0.000655         164         4           open</span><br><span class="line"> 10.49    0.000112          11        10           mmap</span><br><span class="line">  6.09    0.000065          16         4           mprotect</span><br><span class="line">  4.96    0.000053          13         4           brk</span><br><span class="line">  3.93    0.000042          14         3           munmap</span><br><span class="line">  3.09    0.000033          11         3           read</span><br><span class="line">  3.00    0.000032           5         6           close</span><br><span class="line">  3.00    0.000032           5         6           fstat</span><br><span class="line">  1.69    0.000018          18         1         1 access</span><br><span class="line">  1.22    0.000013          13         1           execve</span><br><span class="line">  0.66    0.000007           7         1           arch_prctl</span><br><span class="line">  0.56    0.000006           6         1           write</span><br><span class="line">  0.00    0.000000           0         1           lseek</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line">100.00    0.001068                    45         1 total</span><br><span class="line"></span><br><span class="line"># strace -cp 18700 统计进程各个系统调用的时间消耗</span><br><span class="line">strace: Process 18700 attached</span><br><span class="line">^Cstrace: Process 18700 detached</span><br><span class="line">% time     seconds  usecs/call     calls    errors syscall</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line"> 34.48    0.002522          10       256           poll</span><br><span class="line"> 34.24    0.002505          12       206           sendto</span><br><span class="line"> 27.66    0.002023           8       257           recvfrom</span><br><span class="line">  3.47    0.000254          64         4           write</span><br><span class="line">  0.15    0.000011          11         1           restart_syscall</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line">100.00    0.007315                   724           total</span><br><span class="line"></span><br><span class="line"># strace -Tp 16266 统计每个系统调用的耗时</span><br><span class="line"># 每行的最后使用&lt;&gt;展示调用的耗时,单位</span><br><span class="line">......</span><br><span class="line">read(6, &quot;5.99 6.79 6.95 6/416 15562\n&quot;, 8191) = 27 &lt;0.000013&gt;</span><br><span class="line">lseek(7, 0, SEEK_SET)                   = 0 &lt;0.000016&gt;</span><br><span class="line">read(7, &quot;cpu  29537917 11 28987112 160043&quot;..., 2048) = 1655 &lt;0.000031&gt;</span><br><span class="line">read(7, &quot;&quot;, 1024)                       = 0 &lt;0.000009&gt;</span><br><span class="line">write(1, &quot;\33[H\33(B\33[mtop - 17:37:32 up 1 day&quot;..., 2048) = 2048 &lt;0.000064&gt;</span><br><span class="line">write(1, &quot;;49mwa,\33(B\33[m\33[39;49m\33[1m  0.0 \33&quot;..., 2048) = 2048 &lt;0.000048&gt;</span><br><span class="line">write(1, &quot;  0.4   0:40.83 /work/servers/ph&quot;..., 2048) = 2048 &lt;0.000039&gt;</span><br><span class="line">write(1, &quot;                                &quot;..., 2048) = 2048 &lt;0.000041&gt;</span><br><span class="line">write(1, &quot;                                &quot;..., 2048) = 2048 &lt;0.000039&gt;</span><br><span class="line">write(1, &quot;                             \33(B&quot;..., 2048) = 2048 &lt;0.000087&gt;</span><br><span class="line">write(1, &quot;20 root      20   0       0     &quot;..., 2048) = 2048 &lt;0.000047&gt;</span><br><span class="line">write(1, &quot; 0:02.32 [migration/1]          &quot;..., 1428) = 1428 &lt;0.000037&gt;</span><br><span class="line">pselect6(1, [0], NULL, NULL, &#123;3, 0&#125;, &#123;[WINCH], 8&#125;</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"># strace -e trace=network ping 127.0.0.1</span><br><span class="line"># -e 选项的几个都还挺常用的,如 network 可以观察网络相关的调用</span><br><span class="line">socket(AF_INET, SOCK_RAW, IPPROTO_ICMP) = -1 EPERM (Operation not permitted)</span><br><span class="line">socket(AF_INET, SOCK_DGRAM, IPPROTO_IP) = 3</span><br><span class="line">connect(3, &#123;sa_family=AF_INET, sin_port=htons(1025), sin_addr=inet_addr(&quot;127.0.0.1&quot;)&#125;, 16) = 0</span><br><span class="line">getsockname(3, &#123;sa_family=AF_INET, sin_port=htons(34221), sin_addr=inet_addr(&quot;127.0.0.1&quot;)&#125;, [16]) = 0</span><br><span class="line">ping: icmp open socket: 不允许的操作</span><br><span class="line">+++ exited with 2 +++</span><br><span class="line"></span><br><span class="line"># strace -tt -s 10 -o test.txt 5912</span><br><span class="line"># 跟踪进程号 5912,以微秒为单位统计系统调用时间,每个字符串只输出前 10 个字符,最后将信息输出到文件 test.txt 中</span><br><span class="line"># -o test.txt 等价于 2&gt;text.txt</span><br></pre></td></tr></table></figure>



<h1 id="traceroute"><a href="#traceroute" class="headerlink" title="traceroute"></a>traceroute</h1><h2 id="安装-2"><a href="#安装-2" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install traceroute</span><br></pre></td></tr></table></figure>



<h2 id="文档-2"><a href="#文档-2" class="headerlink" title="文档"></a>文档</h2><p>traceroute 命令用于追踪数据包在网络传输过程中的路径</p>
<p>traceroute 输出每行表示一个网关,每行包含三个时间</p>
<p>有时候会看到 traceroute 输出的网关信息是 * 代替,这是因为该网关防火墙屏蔽了 ICMP 的信息,也表示该网关禁 ping 了</p>
<p>traceroute 实现的方法是使用一个小ttl(生存时间)启动探测数据包，然后侦听来自网关的ICMP超时回复，它以ttl为1开始探测，并将其增加1，直到获得ICMP port unreachable或TCP reset，这意味着我们到达了host，或达到了最大值(默认为30跳)，traceroute 默认每次向目标发送三个包，并将每个探测包到达跳点的时间打印成一行。在请求时地址后面可以有附加信息，如果探测结果来自不同的网关，则会打印每个响应系统的地址，如果在5.0秒内(默认值)没有响应，则会为该探测器打印一个 * 。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Usage:</span><br><span class="line">  traceroute [ -46dFITnreAUDV ] [ -f first_ttl ] [ -g gate,... ] [ -i device ] [ -m max_ttl ] [ -N squeries ] [ -p port ] [ -t tos ] [ -l flow_label ] [ -w waittime ] [ -q nqueries ] [ -s src_addr ] [ -z sendwait ] [ --fwmark=num ] host [ packetlen ]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -4                          Use IPv4</span><br><span class="line">  -6                          Use IPv6</span><br><span class="line">  -d  --debug                 Enable socket level debugging</span><br><span class="line">  -F  --dont-fragment         Do not fragment packets</span><br><span class="line">  -f first_ttl  --first=first_ttl</span><br><span class="line">                             Start from the first_ttl hop (instead from 1)</span><br><span class="line">  -g gate,...  --gateway=gate,...</span><br><span class="line">                             Route packets through the specified gateway</span><br><span class="line">                             (maximum 8 for IPv4 and 127 for IPv6)</span><br><span class="line">  -I  --icmp                  Use ICMP ECHO for tracerouting</span><br><span class="line">  -T  --tcp                   Use TCP SYN for tracerouting (default port is 80)</span><br><span class="line">  -i device  --interface=device</span><br><span class="line">                             Specify a network interface to operate with</span><br><span class="line">  -m max_ttl  --max-hops=max_ttl</span><br><span class="line">                             Set the max number of hops (max TTL to be</span><br><span class="line">                             reached). Default is 30</span><br><span class="line">  -N squeries  --sim-queries=squeries</span><br><span class="line">                             Set the number of probes to be tried</span><br><span class="line">                             simultaneously (default is 16)</span><br><span class="line">  -n                          Do not resolve IP addresses to their domain names</span><br><span class="line">                               直接使用IP地址而非主机名称</span><br><span class="line">  -p port  --port=port        Set the destination port to use. It is either</span><br><span class="line">                             initial udp port value for &quot;default&quot; method</span><br><span class="line">                             (incremented by each probe, default is 33434), or</span><br><span class="line">                             initial seq for &quot;icmp&quot; (incremented as well,</span><br><span class="line">                             default from 1), or some constant destination</span><br><span class="line">                             port for other methods (with default of 80 for</span><br><span class="line">                             &quot;tcp&quot;, 53 for &quot;udp&quot;, etc.)</span><br><span class="line">  -t tos  --tos=tos           Set the TOS (IPv4 type of service) or TC (IPv6</span><br><span class="line">                             traffic class) value for outgoing packets</span><br><span class="line">  -l flow_label  --flowlabel=flow_label</span><br><span class="line">                             Use specified flow_label for IPv6 packets</span><br><span class="line"></span><br><span class="line">  -w waittime  --wait=waittime</span><br><span class="line">                             Set the number of seconds to wait for response to</span><br><span class="line">                             a probe (default is 5.0). Non-integer (float</span><br><span class="line">                             point) values allowed too</span><br><span class="line"></span><br><span class="line">  -q nqueries  --queries=nqueries</span><br><span class="line">                             Set the number of probes per each hop. Default is3</span><br><span class="line"></span><br><span class="line">  -r                          Bypass the normal routing and send directly to a</span><br><span class="line">                             host on an attached network</span><br><span class="line">                             忽略普通的Routing Table，直接将数据包送到远端主机上</span><br><span class="line"></span><br><span class="line">  -s src_addr  --source=src_addr</span><br><span class="line">                             Use source src_addr for outgoing packets</span><br><span class="line"></span><br><span class="line">  -z sendwait  --sendwait=sendwait</span><br><span class="line">                             Minimal time interval between probes (default 0).</span><br><span class="line">                             If the value is more than 10, then it specifies a</span><br><span class="line">                             number in milliseconds, else it is a number of</span><br><span class="line">                             seconds (float point values allowed too)</span><br><span class="line"></span><br><span class="line">  -e  --extensions            Show ICMP extensions (if present), including MPLS</span><br><span class="line"></span><br><span class="line">  -A  --as-path-lookups       Perform AS path lookups in routing registries and</span><br><span class="line">                             print results directly after the corresponding</span><br><span class="line">                             addresses</span><br><span class="line"></span><br><span class="line">  -M name  --module=name      Use specified module (either builtin or external)</span><br><span class="line">                             for traceroute operations. Most methods have</span><br><span class="line">                             their shortcuts (`-I&#x27; means `-M icmp&#x27; etc.)</span><br><span class="line"></span><br><span class="line">  -O OPTS,...  --options=OPTS,...</span><br><span class="line">                             Use module-specific option OPTS for the</span><br><span class="line">                             traceroute module. Several OPTS allowed,</span><br><span class="line">                             separated by comma. If OPTS is &quot;help&quot;, print info</span><br><span class="line">                             about available options</span><br><span class="line"></span><br><span class="line">  --sport=num                 Use source port num for outgoing packets. Implies</span><br><span class="line">                             `-N 1&#x27;</span><br><span class="line"></span><br><span class="line">  --fwmark=num                Set firewall mark for outgoing packets</span><br><span class="line">  </span><br><span class="line">  -U  --udp                   Use UDP to particular port for tracerouting</span><br><span class="line">                             (instead of increasing the port per each probe),</span><br><span class="line">                             default port is 53</span><br><span class="line"></span><br><span class="line">  -UL                         Use UDPLITE for tracerouting (default dest port</span><br><span class="line">                             is 53)</span><br><span class="line"></span><br><span class="line">  -D  --dccp                  Use DCCP Request for tracerouting (default port</span><br><span class="line">                             is 33434)</span><br><span class="line"></span><br><span class="line">  -P prot  --protocol=prot    Use raw packet of protocol prot for tracerouting</span><br><span class="line"></span><br><span class="line">  --mtu                       Discover MTU along the path being traced. Implies</span><br><span class="line">                             `-F -N 1&#x27;</span><br><span class="line"></span><br><span class="line">  --back                      Guess the number of hops in the backward path and</span><br><span class="line">                             print if it differs</span><br><span class="line"></span><br><span class="line">  -V  --version               Print version info and exit</span><br><span class="line"></span><br><span class="line">  --help                      Read this help and exit</span><br><span class="line"></span><br><span class="line">Arguments:</span><br><span class="line"></span><br><span class="line">\+     host          The host to traceroute to</span><br><span class="line">     packetlen     The full packet length (default is the length of an IP</span><br><span class="line">                   header plus 40). Can be ignored or increased to a minimal</span><br><span class="line">                   allowed value</span><br></pre></td></tr></table></figure>



<h2 id="常用命令-2"><a href="#常用命令-2" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 追踪数据包到百度的传输路径,途中的机器一律使用 IP 显示</span><br><span class="line">traceroute -n www.baidu.com</span><br><span class="line"></span><br><span class="line"># 跳过中转直接向目标发送数据包,注意如果是内网会得到[connect: 网络不可达]的错误</span><br><span class="line">traceroute -r www.baidu.com</span><br><span class="line"></span><br><span class="line"># 限定使用 IPV4/6 协议跟踪路由,默认是自动选择,此时如果目标主机返回了 IPV4 和 6 的地址优先使用 IPV4 的</span><br><span class="line">traceroute -4 www.baidu.com</span><br><span class="line">traceroute -6 www.baidu.com</span><br></pre></td></tr></table></figure>



<h1 id="lsof"><a href="#lsof" class="headerlink" title="lsof"></a>lsof</h1><h2 id="安装-3"><a href="#安装-3" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum -y install lsof</span><br></pre></td></tr></table></figure>



<h2 id="文档-3"><a href="#文档-3" class="headerlink" title="文档"></a>文档</h2><p>lsof 命令用于查看系统进程打开的文件</p>
<p>在 linux 环境下一切皆文件,系统为应用程序分配文件描述符作为应用程序和操作系统间交互的桥梁.而应用程序打开的文件的描述符列表则通过 lsof 命令提供了大量关于应用程序的信息,可用于系统监测和排错</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lsof 4.87</span><br><span class="line"></span><br><span class="line"> latest revision: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/</span><br><span class="line"> latest FAQ: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/FAQ</span><br><span class="line"> latest man page: ftp://lsof.itap.purdue.edu/pub/tools/unix/lsof/lsof_man</span><br><span class="line"> usage: [-?abhKlnNoOPRtUvVX] [+|-c c] [+|-d s] [+D D] [+|-f[gG]] [+|-e s]</span><br><span class="line"> [-F [f]] [-g [s]] [-i [i]] [+|-L [l]] [+m [m]] [+|-M] [-o [o]] [-p s]</span><br><span class="line">[+|-r [t]] [-s [p:s]] [-S [t]] [-T [t]] [-u s] [+|-w] [-x [fl]] [--] [names]</span><br><span class="line"></span><br><span class="line">Defaults in parentheses; comma-separated set (s) items; dash-separated ranges.</span><br><span class="line">  -?|-h list help          -a AND selections (OR)     -b avoid kernel blocks</span><br><span class="line">  -c c  cmd c ^c /c/[bix]  +c w  COMMAND width (9)    +d s  dir s files</span><br><span class="line">  -d s  select by FD set   +D D  dir D tree *SLOW?*   +|-e s  exempt s *RISKY*</span><br><span class="line">  -i select IPv[46] files  -K list tasKs (threads)    -l list UID numbers</span><br><span class="line">  -n no host names         -N select NFS files        -o list file offset</span><br><span class="line">  -O no overhead *RISKY*   -P no port names           -R list paRent PID</span><br><span class="line">  -s list file size        -t terse listing           -T disable TCP/TPI info</span><br><span class="line">  -U select Unix socket    -v list version info       -V verbose search</span><br><span class="line">  +|-w  Warnings (+)       -X skip TCP&amp;UDP* files     -Z Z  context [Z]</span><br><span class="line">  </span><br><span class="line">  -- end option scan</span><br><span class="line">  +f|-f  +filesystem or -file names     +|-f[gG] flaGs</span><br><span class="line">  -F [f] select fields; -F? for help</span><br><span class="line">  +|-L [l] list (+) suppress (-) link counts &lt; l (0 = all; default = 0)</span><br><span class="line">                                       +m [m] use|create mount supplement</span><br><span class="line">  +|-M   portMap registration (-)       -o o   o 0t offset digits (8)</span><br><span class="line">  -p s   exclude(^)|select PIDs         -S [t] t second stat timeout (15)</span><br><span class="line">  -T qs TCP/TPI Q,St (s) info</span><br><span class="line">  -g [s] exclude(^)|select and print process group IDs</span><br><span class="line">  -i i   select by IPv[46] address: [46][proto][@host|addr][:svc_list|port_list]</span><br><span class="line">  +|-r [t[m&lt;fmt&gt;]] repeat every t seconds (15);  + until no files, - forever.</span><br><span class="line">      An optional suffix to t is m&lt;fmt&gt;; m must separate t from &lt;fmt&gt; and</span><br><span class="line">     &lt;fmt&gt; is an strftime(3) format for the marker line.</span><br><span class="line">  -s p:s  exclude(^)|select protocol (p = TCP|UDP) states by name(s).</span><br><span class="line">  -u s   exclude(^)|select login|UID set s</span><br><span class="line">  -x [fl] cross over +d|+D File systems or symbolic Links</span><br><span class="line">  names  select named files or files on named file systems</span><br><span class="line">Anyone can list all files; /dev warnings disabled; kernel ID check disabled.</span><br></pre></td></tr></table></figure>



<h2 id="常用命令-3"><a href="#常用命令-3" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 查看某进程打开的文件</span><br><span class="line">lsof -p pid</span><br><span class="line"></span><br><span class="line"># 查看正在使用某端口进程</span><br><span class="line">lsof -i:80</span><br><span class="line"></span><br><span class="line"># 列出打开文件的进程</span><br><span class="line">lsof $filename</span><br></pre></td></tr></table></figure>



<h1 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h1><p>网上有看到关于[因为 lsof 需要访问系统核心文件所以需要 root 用户执行]的说法,自己试了下普通用户也可以,可能跟具体需要访问的文件权限有关,具体需要深入了解才能确定了。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.computerhope.com/unix/utracero.htm#desc">computerhope 的 traceroute 介绍</a></p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>由 map 深入理解 yield 和生成器</title>
    <url>/2023/02/38446edf/</url>
    <content><![CDATA[<p>首先看下 map 方法的使用代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    map_iter = <span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * x, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="built_in">print</span>([i <span class="keyword">for</span> i <span class="keyword">in</span> map_iter])</span><br><span class="line">    <span class="built_in">print</span>([i <span class="keyword">for</span> i <span class="keyword">in</span> map_iter])</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[1, 4, 9]</span><br><span class="line">[]</span><br></pre></td></tr></table></figure>

<p>可以看到输出两次 map_iter 中的值只有第一次正常获取了，后面拿到的都是空。这里是因为 map 内部的 yield 使用。</p>
<h1 id="map-中的-yield"><a href="#map-中的-yield" class="headerlink" title="map 中的 yield"></a>map 中的 yield</h1><blockquote>
<p>map(func, list[ , list]) 列表中的 list 依次调用函数 func</p>
</blockquote>
<p>查阅文档 <a href="https://docs.python.org/3/library/functions.html#map">Python3 文档关于 map 的解释</a> 可以看到</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For cases where the function inputs are already arranged into argument tuples, see itertools.starmap().</span></span><br><span class="line"><span class="comment"># 当函数输入形如元组时，参见 itertools.starmap().</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The difference between map() and starmap() parallels the distinction between function(a,b) and function(*c)</span></span><br><span class="line"><span class="comment"># map() 和 starmap() 的区别是参数形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># starmap 源码示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">starmap</span>(<span class="params">function, iterable</span>):</span><br><span class="line">    <span class="comment"># starmap(pow, [(2,5), (3,2), (10,3)]) --&gt; 32 9 1000</span></span><br><span class="line">    <span class="keyword">for</span> args <span class="keyword">in</span> iterable:</span><br><span class="line">        <span class="keyword">yield</span> function(*args)</span><br></pre></td></tr></table></figure>

<p>由此我们明确了 map 方法实现基于 yield，那么为什么 yield 会导致 map 方法返回值只能遍历一次呢</p>
<h2 id="了解-yield"><a href="#了解-yield" class="headerlink" title="了解 yield"></a>了解 yield</h2><p>包含 yield 关键字的函数称为生成器，生成器是 Python 新引入的概念。</p>
<p>生成器长得很像普通函数，看上去几乎只是 return 被 yield 代替的感觉。但生成器是一种迭代器，行为跟普通函数有很大区别。</p>
<p>生成器函数被调用时，函数体的代码不会被执行，而是返回一个迭代器。每请求一次迭代器（g.next() 或 g.__next__()）就执行一次生成器中的代码，直到遇到 yield 和 return。当遇到 yield 时生成一个值输出，然后冻结函数等待下次请求后从冻结的代码处（yield 下一行）继续执行。当遇到 return 时意味着停止该生成器。</p>
<p>yield 代码使用示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = (i**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">g.__next__()</span><br><span class="line"><span class="comment"># output: 4</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">sum</span>(i**<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"><span class="comment"># output: 13</span></span><br></pre></td></tr></table></figure>

<p>由此我们理解了 yield 关键在作用是定义生成器，也知道了生成器的执行原理，那么生成器是什么呢？如此我们需要了解可迭代对象、迭代器和生成器。</p>
<h1 id="可迭代对象"><a href="#可迭代对象" class="headerlink" title="可迭代对象"></a>可迭代对象</h1><blockquote>
<p>如果一个对象拥有 __iter__ 方法，则称为可迭代对象</p>
</blockquote>
<p>对象是否包含 __iter__ 可以通过 dir(object) 检查，我们日常利用循环遍历的对象都是可迭代对象，常见的有列表、元组、字典。可以参见另一篇关于 python.for 循环的博客。</p>
<p>以下是简单的实现可迭代对象代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyIterable</span>(<span class="title class_ inherited__">object</span>):  <span class="comment"># 定义可迭代对象类</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> MyIterator(self.data)  <span class="comment"># 返回该可迭代对象的迭代器类的实例</span></span><br></pre></td></tr></table></figure>



<h2 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h2><blockquote>
<p>如果一个可迭代对象拥有 __next__ 函数，则为迭代器</p>
</blockquote>
<p>注：__next__ 方法在 Python 2.x 是 new，见 <a href="https://peps.python.org/pep-3114/">PEP 3114</a></p>
<p>另外需要注意的是 __iter__ 方法要求返回的是具体的迭代器实例。所以在可迭代对象中返回的是对应的迭代器实例，在迭代器中则只需要返回自身。</p>
<p>以下是简单的迭代器代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyIterator</span>(<span class="title class_ inherited__">object</span>):  <span class="comment"># 定义迭代器类，其是MyList可迭代对象的迭代器类</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num</span>):</span><br><span class="line">        self.up = num  <span class="comment"># 上边界</span></span><br><span class="line">        self.now = <span class="number">0</span>  <span class="comment"># 当前迭代值，初始为0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self  <span class="comment"># 返回该对象的迭代器类的实例；因为自己就是迭代器，所以返回 self</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>):  <span class="comment"># 迭代器类必须实现的方法</span></span><br><span class="line">        <span class="keyword">while</span> self.now &lt; self.up:</span><br><span class="line">            self.now += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> self.now - <span class="number">1</span>  <span class="comment"># 返回当前迭代值</span></span><br><span class="line">        <span class="keyword">raise</span> StopIteration  <span class="comment"># 超出上边界，抛出异常</span></span><br></pre></td></tr></table></figure>



<h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><blockquote>
<p>生成器则是一种特殊的迭代器，使用 yield 关键字的函数被称为生成器</p>
</blockquote>
<p>理解生成器需要注意：</p>
<ol>
<li>生成器支持了更多操作方法（python &gt;&#x3D; 2.5， <a href="https://peps.python.org/pep-0342/">PEP 342</a>），比如允许在迭代过程中修改当前迭代值（生成器与迭代器的主要区别）</li>
<li>虽然也可以通过类模拟生成器的逻辑，但生成器的定义是使用 yield 关键字</li>
</ol>
<p>以下是一段生成器示例代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">myGenerator</span>(<span class="params">num</span>):  <span class="comment"># 定义生成器</span></span><br><span class="line">    now = <span class="number">0</span>  <span class="comment"># 当前迭代值，初始为0</span></span><br><span class="line">    <span class="keyword">while</span> now &lt; num:</span><br><span class="line">        val = (<span class="keyword">yield</span> now)  <span class="comment"># 返回当前迭代值，并接受可能的send发送值；yield在下面会解释</span></span><br><span class="line">        now = now + <span class="number">1</span> <span class="keyword">if</span> val <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> val  <span class="comment"># val为None，迭代值自增1，否则重新设定当前迭代值为val</span></span><br><span class="line">    <span class="keyword">raise</span> StopIteration</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_g = myGenerator(<span class="number">5</span>)  <span class="comment"># 得到一个生成器对象</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(my_g.__next__()) <span class="comment"># 返回当前迭代值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_g)) <span class="comment"># 等价 my_g.__next__()</span></span><br><span class="line"></span><br><span class="line">my_g.send(<span class="number">3</span>)  <span class="comment"># 重新设定当前的迭代值，Python &gt;= 2.5</span></span><br><span class="line"><span class="built_in">print</span>(my_g.__next__())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(my_g))  <span class="comment"># 返回该对象所拥有的方法名，可以看到__iter__与__next__在其中</span></span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">➜  python python3 study/generator.py</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">[&#x27;__class__&#x27;, &#x27;__del__&#x27;, &#x27;__delattr__&#x27;, &#x27;__dir__&#x27;, &#x27;__doc__&#x27;, &#x27;__eq__&#x27;, &#x27;__format__&#x27;, &#x27;__ge__&#x27;, &#x27;__getattribute__&#x27;, &#x27;__gt__&#x27;, &#x27;__hash__&#x27;, &#x27;__init__&#x27;, &#x27;__init_subclass__&#x27;, &#x27;__iter__&#x27;, &#x27;__le__&#x27;, &#x27;__lt__&#x27;, &#x27;__name__&#x27;, &#x27;__ne__&#x27;, &#x27;__new__&#x27;, &#x27;__next__&#x27;, &#x27;__qualname__&#x27;, &#x27;__reduce__&#x27;, &#x27;__reduce_ex__&#x27;, &#x27;__repr__&#x27;, &#x27;__setattr__&#x27;, &#x27;__sizeof__&#x27;, &#x27;__str__&#x27;, &#x27;__subclasshook__&#x27;, &#x27;close&#x27;, &#x27;gi_code&#x27;, &#x27;gi_frame&#x27;, &#x27;gi_running&#x27;, &#x27;gi_yieldfrom&#x27;, &#x27;send&#x27;, &#x27;throw&#x27;]</span><br><span class="line">➜  python </span><br></pre></td></tr></table></figure>

<p>需要注意的是 send 需要在生成器挂起后（也就是先触发一次 yield）才有意义，否则会报错</p>
<p>TypeError: can’t send non-None value to a just-started generator</p>
<p>除此以外生成器在 Python &gt;&#x3D; 2.5 还支持 throw 和 close 方法，作用分别是在生成器内的 yield 表达式中抛出一个异常，和停止生成器，具体可以看 <a href="https://peps.python.org/pep-0342/#new-generator-method-close">PEP 342</a></p>
<p>关于生成器在 python 底层的运行逻辑，知乎有大佬做了解释：<a href="https://zhuanlan.zhihu.com/p/94126166">yield! 深入理解 Python 最甜美的特性！</a>，后面关于【再次遇到 yield value2】 的逻辑没看明白，等我完全搞明白了自己写一个整理。。。</p>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://docs.python.org/3/library/functions.html#map">Python3 文档关于 map 的解释</a></p>
<p><a href="https://peps.python.org/pep-0342/#new-generator-method-close">PEP 342 生成器增强</a></p>
<p>《Python 基础教程（第二版·修订版）》</p>
<p><a href="https://zhuanlan.zhihu.com/p/94126166">yield! 深入理解 Python 最甜美的特性！</a></p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL事务</title>
    <url>/2023/02/f91535f/</url>
    <content><![CDATA[<h1 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h1><p>事务拥有四大特性，原子性、一致性、隔离性、持久性。</p>
<ul>
<li><p>原子性：事务的所有操作，要么都执行，要么都不执行</p>
</li>
<li><p>一致性：事务执行前后，数据完整性没有被破坏。意思是指事务的操作不会破坏任何 MySQL 规则，如唯一索引约束等。</p>
</li>
<li><p>隔离性：多个事务并行执行时，最终呈现的效果是串行的。意思是事务执行过程中，只能感知到自己操作引起的数据变化，不会查询到自己之后的其它事务更改的数据。</p>
</li>
<li><p>持久性：一旦事务提交，对数据库的修改将是永久的，不会因为系统故障等丢失</p>
</li>
</ul>
<span id="more"></span>



<h2 id="MySQL-如何保证四大特性"><a href="#MySQL-如何保证四大特性" class="headerlink" title="MySQL 如何保证四大特性"></a>MySQL 如何保证四大特性</h2><p>原子性：undo log</p>
<p>一致性：程序故障 + redo log + undo log</p>
<p>隔离性：undo log + 事务（ID+隔离级别） + mvcc + 锁</p>
<p>持久性：redo log</p>
<h1 id="隔离级别"><a href="#隔离级别" class="headerlink" title="隔离级别"></a>隔离级别</h1><blockquote>
<p>各个级别的定义、可能存在的问题</p>
</blockquote>
<p>在并发的情境下，多个事务并行执行总会产生各种奇怪的问题，事务隔离级别就是为了解构这些情景被提出的。</p>
<p>不同的隔离级别体现的事务隔离性不同，宽松的隔离级别并发度高但容易产生奇怪的数据，严格的隔离级别并发度低但是能保证数据符合预期。</p>
<p>事务隔离级别有：</p>
<ul>
<li>读未提交：事务未提交时，它的修改就可以被其它事务读取到，存在脏读问题</li>
<li>读提交：只有事务提交后，它的修改才会被其它事务读取到</li>
<li>可重复读（REPAETABLE READ）：事务前后读取相同数据行，得到的信息前后一致。</li>
<li>串行化：对同一行记录而言，事务总是串行进行</li>
</ul>
<p>不同的事务隔离级别存在不同的问题，如：</p>
<ul>
<li><p>读未提交 - 脏读：读取到了其它事务未提交的数据，事务 A 在事务 B 读取数据时刚好修改了数据，B 读取后 A 反而回滚了事务，就使得事务 B 读取到了本不应该存在的数据！</p>
</li>
<li><p>读提交 - 不可重复读：一个事务前后读取某数据，得到的结果不一致</p>
</li>
<li><p>All - 幻读：事务前后读取的数据量不一致，幻读专指 insert 的情况</p>
</li>
</ul>
<p>Oracle、SQLserver、PostgreSQL 数据库的默认隔离级别都是读提交，MySQL 的默认隔离级别是可重复读。</p>
<p>查看事务隔离级别：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># MySQL &gt;= 5.7.20 是 transaction_isolation，之前和 mariadb 是 tx_isolation</span><br><span class="line">show variables like &#x27;transaction_isolation&#x27;;</span><br></pre></td></tr></table></figure>



<p>数据库为了实现以上数据库隔离级别，通过在事务执行前创建一个一致性视图，控制事务读取的逻辑结果。</p>
<p>读未提交时，不创建数据视图；</p>
<p>读提交时，事务的每个 SQL 执行时都创建一个自己用的数据视图；</p>
<p>可重复读时，视图在事务启动时创建，整个事务操作都用这个视图；</p>
<p>串行化也没有数据视图，直接通过加锁的形式避免同时操作同一行。</p>
<h2 id="可重复读的实现"><a href="#可重复读的实现" class="headerlink" title="可重复读的实现"></a>可重复读的实现</h2><p>一致性视图和行锁从两个不同的方面保证了事务的原子性和隔离性。</p>
<p>可重复读的核心是一致性读（consistent read）。首先，一致性视图使得事务读操作变为快照读，保证每次读取得到的都是事务启动时的数据版本，而当事务内部更新了某个字段的数据，因为也会记录当前事务版本号（和 undo log）到数据行，所以能读取到自己修改的信息。而行锁通过写写互斥保证了事务的写操作不会被其他事务中断。</p>
<p>而事务更新时，只会当前读。如果要读取的数据行被其它事务持有行锁，进入锁等待。</p>
<h3 id="使用可重复读的场景"><a href="#使用可重复读的场景" class="headerlink" title="使用可重复读的场景"></a>使用可重复读的场景</h3><blockquote>
<p>当前查询操作不希望受到实时更新影响的时候</p>
</blockquote>
<p>假设我们在管理一个个人银行账户表。一个表存了账户余额，一个表存了账单明细。到了月底要做数据校对，需要判断上个月的余额和当前余额的差值是否与本月的账单明细一致。我们一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响校对结果。</p>
<p>这时候使用“可重复读”隔离级别就很方便。事务启动时的视图被认为是静态的，事务执行过程中不受其他事务更新的影响。</p>
<h1 id="长事务的弊端"><a href="#长事务的弊端" class="headerlink" title="长事务的弊端"></a>长事务的弊端</h1><blockquote>
<p>长事务会导致系统中大量一致性视图、undo log 和锁资源无法释放</p>
</blockquote>
<p>长事务是指启动和提交间隔时间很长的事务。长事务会导致系统存在大量 read-view 和 undo log，占据大量内存空间，除此以外长事务还会占用锁资源，严重威胁并发甚至可能拖垮数据库。</p>
<h2 id="连续事务衔接"><a href="#连续事务衔接" class="headerlink" title="连续事务衔接"></a>连续事务衔接</h2><p>当连续执行多个事务时，建议使用 commit work and chain 启动事务。当 autocommit &#x3D; 1 时使用 begin 显式启动事务，然后执行 commit work and chan 代替单纯的 commit 提交事务，此操作可以在提交事务的同时自动启动下一个事务，减少一次 begin 事务的交互。</p>
<h2 id="事务优化"><a href="#事务优化" class="headerlink" title="事务优化"></a>事务优化</h2><p>优化的角度可以有很多：</p>
<ul>
<li><p>从监控的角度可以</p>
<ol>
<li>通过 information_schema.innodb_trx 监控事务的持续时间</li>
<li>通过 pt 工具监控事务的持续时间</li>
</ol>
</li>
<li><p>从配置角度可以</p>
<ol>
<li>增加 undo log 空间，将 innodb_undo_tablespace 设置为 2 或更大的值</li>
<li>通过配置 max_execution_time 控制事务最大执行时间</li>
</ol>
</li>
<li><p>还可以想办法减少长事务的执行时间</p>
<ol>
<li>从优化语句的角度入手，比如优化索引</li>
<li>将不必要的大批量查询导致的长事务拆分为多个事务</li>
</ol>
</li>
</ul>
<h1 id="MVCC"><a href="#MVCC" class="headerlink" title="MVCC"></a>MVCC</h1><p>MVCC 全称多版本并发控制。每个事务启动时（RR）生成一个一致性视图，通过一致性视图声明：承认在事务启动前的数据版本（和事务自己的修改），不承认事务启动后数据发生的改变。</p>
<h2 id="一致性视图"><a href="#一致性视图" class="headerlink" title="一致性视图"></a>一致性视图</h2><p>MVCC 在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。这个视图数组和高低水位，组成当前事务的一致性视图（read-view）。</p>
<p>对当前事务的启动瞬间而言，每个数据版本的 row trx_id，有以下几种可能：</p>
<ol>
<li>小于等于低水位事务 ID，表示这个版本是已提交的事务或者是当前事务自己生成的，数据可见；</li>
<li>大于高水位事务 ID，表示这个版本是由将来启动的事务生成的，不可见；</li>
<li>在高低水位事务 ID 之间，那就包括两种情况：<ol>
<li>若 row trx_id 在事务 ID 数组中，表示这个版本由未提交事务生成，不可见；</li>
<li>若 row trx_id 不在事务 ID 数组中，表示这个版本由已提交事务生成，可见。</li>
</ol>
</li>
</ol>
<h2 id="当前读"><a href="#当前读" class="headerlink" title="当前读"></a>当前读</h2><p>因为数据库更新当然是在最新版本基础上更新，但是仅有一致性视图的话可能导致在之前的某个版本基础上更新！</p>
<p>为此设计了新的规则：update 操作在存储引擎内部进行先读后写，这个读就是当前读！顾名思义，就是永远只会读取到最新的数据！</p>
<p>此外，select 语句通过加锁也可以做到当前读：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select k from t where id=1 lock in share mode;</span><br><span class="line"># 或</span><br><span class="line">select k from t where id=1 for update;</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL日志系统</title>
    <url>/2023/02/964962dd/</url>
    <content><![CDATA[<h1 id="重做日志-redo-log"><a href="#重做日志-redo-log" class="headerlink" title="重做日志 redo log"></a>重做日志 redo log</h1><blockquote>
<p>redo log buffer 和 redo log file、循环写、innodb_flush_log_at_trx_commit、crash-safe</p>
</blockquote>
<p>redo log 是 innodb 特有的，为了优化 MySQL 每次更新操作都要直接操作磁盘带来的 IO 压力诞生。redo log 是物理日志，记录的是【在某个数据页上做了什么操作】。流程就是每次更新先写 redo log，等必要时候(系统空闲或 redo log 写满)再将 redo log 信息写入磁盘，这项技术就是常说的 WAL 技术，全称 Write-Ahead Logging。</p>
<p>redo log 分为两部分：</p>
<ul>
<li>内存中的日志缓冲 redo log buffer</li>
<li>磁盘中的重做日志文件 redo log file</li>
</ul>
<h2 id="redo-log-file"><a href="#redo-log-file" class="headerlink" title="redo log file"></a>redo log file</h2><p>一般情况说的 redo log 就是指 redo log file。需要注意的是：redo log file 即使写在磁盘上也比不使用 redo log 效率更高。原因是 redo log file 操作本质是 CPU 顺序磁盘 IO，比更新操作的随机 IO 快得多。redo log file 的配置是：</p>
<p>innodb_log_file_size 控制每个 redo log file 文件大小；</p>
<p>innodb_log_file_in_group 控制 redo log file 文件个数。</p>
<p>redo log file 的写入方式是循环写，类似循环链表，空间固定会用尽–触发 flush 操作。</p>
<h2 id="crash-safe"><a href="#crash-safe" class="headerlink" title="crash-safe"></a>crash-safe</h2><p>redo log file 的存在使得 innodb 即使发生数据库异常重启，之前提交的记录也不会丢失，这个能力叫 crash-safe。但是并不是有 redo log 就一定获得了这个能力，innodb_flush_log_at_trx_commit 配置就是决定是否拥有 crash-safe 的。</p>
<p>上面说到 redo log 记录也有细分 redo log buffer 和 redo log file，而 innodb_flush_log_at_trx_commit 就是控制事务提交时 redo log 记录策略的，取值含义如下：</p>
<p>取 0：提交事务时只保证写入了 redo log buffer，buffer -&gt; file 的过程由 MySQL 主线程每秒执行一次刷新磁盘处理。</p>
<p>取 1：提交事务时保证 redo log 已经写入磁盘的 redo log file 中。</p>
<p>取 2：提交事务时保证 redo log 已经写入磁盘的 OS Cache 中，由系统处理 OS Cache -&gt; file 的过程，相当于 0 和 1 的中间态。</p>
<p>由此可知，只有 innodb_flush_log_at_trx_commit &#x3D; 1 时才真正拥有 crash-safe 能力。0 和 2 都有可能异常重启时丢失数据，不过因为系统写入磁盘的操作 fsync() 是阻塞的，所以 1 的性能会显著低于同行。</p>
<h1 id="归档日志-binlog"><a href="#归档日志-binlog" class="headerlink" title="归档日志 binlog"></a>归档日志 binlog</h1><blockquote>
<p>MySQL Server 层、追加写、sync_binlog</p>
</blockquote>
<p>binlog 是 MySQL Server 层支持的所有存储引擎都能用，它是逻辑日志，记录的是【操作的原始逻辑，比如给 ID &#x3D; 1 这一行的 a 字段 +1】。</p>
<p>binlog 是追加写，binlog 日志文件达到一定程度后会切换写入下一个文件，每次写入操作顺序追加，并不会覆盖之前的记录。</p>
<p>因为 crash 发生时 binlog 不能保证其中的日志都已经被更新到磁盘，故没有 redo log 的 crash-safe 能力。</p>
<h2 id="sync-binlog"><a href="#sync-binlog" class="headerlink" title="sync_binlog"></a>sync_binlog</h2><p>binlog 日志记录也有类似 redo log 的 buffer 和 file。MySQL 的 sync_binlog 配置控制着 binlog 日志写入磁盘的逻辑，并与 innodb_flush_log_at_trx_commit 类似。取值含义如下：</p>
<p>取 0：事务提交时只保证写入了 OS Cache，cache -&gt; file 的过程由操作系统管理</p>
<p>取 1：事务提交前即保证 binlog 日志写入到磁盘中</p>
<p>取 N，N 为非 0 或 1 的非负整数：当收集到 N 个 binlog 日志后再进一次性写入文件操作</p>
<p>显而易见，0 和 N 都在不同角度尝试提交性能。取 1 则性能差不少，但在服务奔溃的情况下，binlog 丢失的事务的 redo log 处于 prepare 状态所以提供了回滚事务的依据。</p>
<p>为了保持事务的一致性，MySQL 官方文档推荐 innodb_flush_log_at_trx_commit 和 sync_binlog 都取 1。</p>
<h1 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a>两阶段提交</h1><p>所谓两阶段提交是保证跨系统(MySQL 中是 Server 层和存储引擎层 innodb)事务提交信息一致性的方案。 MySQL 中两阶段提交处理事务操作如下：</p>
<p>假设语句： update table_1 set a &#x3D; a + 1 where id &#x3D; 1</p>
<ol>
<li>执行器先调用存储引擎取 id &#x3D; 1 这一行数据，存储引擎将 id &#x3D; 1 这一行所在数据页 A 加载到内存中，并返回 id &#x3D; 1 的行记录信息</li>
<li>执行器拿到返回的 id &#x3D; 1 记录，将 a 列执行 +1 操作，得到新的记录行，再调用存储引擎接口写新行数据</li>
<li>存储引擎将数据写入上述内存空间中的数据页 A，然后将更新操作记录到 redo log 中【将 A 中 m 字节到 n 字节部分的数据内容更新为新数据】。此时 redo log 处于 prepare 状态。然后存储引擎告知执行器执行完成，可以提交事务。</li>
<li>执行器生成并记录 binlog</li>
<li>执行器再次调起存储引擎接口，指示提交事务，引擎将刚刚的 redo log 从 prepare 状态修改为 commit 状态，完成。</li>
</ol>
<h1 id="回滚日志-undo-log"><a href="#回滚日志-undo-log" class="headerlink" title="回滚日志 undo log"></a>回滚日志 undo log</h1><blockquote>
<p>作用、删除时机</p>
</blockquote>
<p>innodb 的每条记录在【更新】时都会记录一条回滚操作日志，方便需要的时候回溯行记录的历史版本，就是 undo log。</p>
<p>注意插入的新纪录是没有 undo log 的，新插入的数据是幻读的问题了，undo log 则是为了实现事务隔离，解决可重复读问题存在。</p>
<p>redo log 和 undo log 都是事务日志，是 innodb 存储引擎层的日志。</p>
<p>undo log 会在系统判断没有任何事务需要用到该条日志记录时被删除。所谓没有任何事务需要用到，是指当前系统没有任何事务的 read-view 比这个 undo log 的版本更早</p>
<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p><a href="https://juejin.cn/post/6895265596985114638">Innodb 引擎:详解 redo log 存储结构</a></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/replication-options-binary-log.html">MySQL 5.7 文档</a></p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL架构</title>
    <url>/2023/02/13b5e6a7/</url>
    <content><![CDATA[<h1 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h1><img src="../../images/1647334224885-6950995.png" alt="MySQL 架构" style="zoom:50%;" />

<p>大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。</p>
<p>Server 层包括连接器、查询缓存、分析器、优化器、执行器等，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。</p>
<p>存储引擎层负责数据的存储和提取。</p>
<p>InnoDB 从 MySQL 5.5.5 版本开始成为默认存储引擎。</p>
<h2 id="连接器"><a href="#连接器" class="headerlink" title="连接器"></a>连接器</h2><p>连接器负责跟客户端建立连接、获取权限、维持和管理连接。</p>
<p>要执行 MySQL 操作首先需要建立 MySQL 连接，例如使用工具或命令行 mysql -h -u -P 等。连接发起后首先会完成经典的 TCP 握手，然后连接器开始认证身份，用的就是输入的用户名和密码。</p>
<p>认证身份顺序是：</p>
<ol>
<li>账密检查</li>
<li>权限检查，主要检查用户(来源IP)是否有链接某库的权限等</li>
</ol>
<p>注意：连接建立后整个链接中执行的操作权限均依赖【权限检查】时读取到的用户权限，意味着建立连接后如果修改了权限需要重新建立 MySQL 链接。</p>
<h3 id="链接自动断开"><a href="#链接自动断开" class="headerlink" title="链接自动断开"></a>链接自动断开</h3><p>链接建立后如果长时间无操作，连接器会自动断开，这个时间由配置 wait_timeout 控制，默认 8 小时。</p>
<h3 id="长链接和短链接"><a href="#长链接和短链接" class="headerlink" title="长链接和短链接"></a>长链接和短链接</h3><p>MySQL 中区分长链接和短链接，顾名思义，长链接指建立连接后一直维持链接不断开，多次操作均使用相同链接。短链接则指每次只执行少数几个操作即断开，下次需要再重新建立连接。</p>
<p>优缺点：</p>
<p>长链接的优点是减少建立连接的资源消耗（扩展，建立连接过程中产生了哪些资源消耗呢？），缺点是内存资源消耗严重，因为 MySQL 的内存管理是基于链接的，申请的内存资源只有断开时才会释放。</p>
<p>链接优缺点则相反。</p>
<p>关于长链接的内存资源使用，后面再整理，目前的知识结构认为内存消耗大致有：执行操作将数据页查出放到内存、保存事务的数据视图、维持链接的信息（存疑）</p>
<p>处理:</p>
<ol>
<li>定时断开长链接</li>
<li>MySQL &gt;&#x3D; 5.7 时，可以通过执行 mysql_reset_connetion 初始化链接资源，此操作不需要断开链接但是会将链接恢复到创建完成的状态。</li>
</ol>
<h2 id="查询缓存"><a href="#查询缓存" class="headerlink" title="查询缓存"></a>查询缓存</h2><p>弊大于利的模块，一般不需要开启。</p>
<p>查询缓存将过往的查询语句和结果以 key-value 的形式存储在内存中，当新查询过来时检查是否存在匹配记录，如果命中则在权限校验后直接返回结果。</p>
<p>但是查询缓存有以下致命限制：</p>
<ol>
<li>查询缓存的匹配判断是【查询语句完全一致】</li>
<li>只要对某表执行 insert&#x2F;update 操作，该表的所有查询缓存将清空</li>
</ol>
<p>由此导致的结果是查询缓存命中率非常低，往往在频繁查询的静态库时使用。</p>
<p>注意：查询缓存功能在 mysql &gt;&#x3D; 8.0 被移除。</p>
<h3 id="使用查询缓存"><a href="#使用查询缓存" class="headerlink" title="使用查询缓存"></a>使用查询缓存</h3><p>MySQL 也提供“按需使用”的方式。可以将参数 query_cache_type 设置成 DEMAND，由此对于默认的 SQL 语句都不使用查询缓存。而对于确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select SQL_CACHE * from T where ID=10；</span><br></pre></td></tr></table></figure>



<h2 id="分析器"><a href="#分析器" class="headerlink" title="分析器"></a>分析器</h2><p>分析器主要做两件事，词法分析(解析器)和语法分析(预处理器)。</p>
<p>词法分析：解释输入操作中的每个词的含义，生成解析树，比如识别 select 关键字，字段名等；</p>
<p>语法分析：根据词法分析得到的语义解析树，判断操作是否满足 MySQL 语法，比如 SQL 是否书写错误、字段是否存在等。</p>
<p>分析器也会做一次权限验证，叫 precheck。主要针对【能否做某个操作】做校验，比如是否有 create table 权限。</p>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>优化器负责选择索引和确定 join 查询的表连接顺序。</p>
<h2 id="执行器"><a href="#执行器" class="headerlink" title="执行器"></a>执行器</h2><p>执行器也会做权限检验，主要针对表级操作，譬如是否有操作某表的权限。</p>
<p>同时执行器会根据表的引擎定义调取对应引擎的接口，搜索引擎的接口诸如“取某个表的第一行”、“取下一行”等，</p>
<h3 id="慢日志的-rows-examined"><a href="#慢日志的-rows-examined" class="headerlink" title="慢日志的 rows_examined"></a>慢日志的 rows_examined</h3><p>在 MySQL 慢日志中有个指标 rows_examined，含义就是执行器调用了搜索引擎取数据行的次数</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>MySQL 基础架构图出自极客林晓斌老师的《Mysql 实战 45 讲》，侵删。</p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL锁</title>
    <url>/2023/02/d940f18d/</url>
    <content><![CDATA[<p>数据库在应对并发访问时需要一套规则合理控制资源访问，而锁就是用来实现控制规则的数据结构。</p>
<span id="more"></span>

<h1 id="全局锁"><a href="#全局锁" class="headerlink" title="全局锁"></a>全局锁</h1><blockquote>
<p>使用场景、跟只读的区别</p>
</blockquote>
<p>顾名思义，全局锁是对整个数据库实例加锁，作用是让整个库处于只读状态。</p>
<p>通过以下语句可以施加全局锁（FTWRL）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">flush tables with read lock</span><br></pre></td></tr></table></figure>



<h2 id="全局锁的使用场景"><a href="#全局锁的使用场景" class="headerlink" title="全局锁的使用场景"></a>全局锁的使用场景</h2><p>全局锁的经典使用场景就是【全库逻辑备份】，也就是把整库查询出来存成文本。</p>
<p>问：全库备份为什么要施加全局锁？</p>
<p>答：发起数据库备份时，不同表备份顺序的先后会有时间差，在这个时间差之内对表的操作有可能造成数据不一致。也就是说数据备份得到的库是逻辑不一致的。</p>
<p>官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数 -–single-transaction 的时候，导数据之前就会启动一个事务确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。</p>
<p>当然，前提是表对应的存储引擎要支持事务，MyISAM 就不支持，也是它的局限之一。</p>
<h2 id="全局锁和设置只读的区别"><a href="#全局锁和设置只读的区别" class="headerlink" title="全局锁和设置只读的区别"></a>全局锁和设置只读的区别</h2><p>乍一看数据库备份也可以通过设置数据库只读（set global readonly &#x3D; true;）来代替，其实他们是有区别的：</p>
<ol>
<li>readonly &#x3D; true 被广泛使用在业务系统中的其它逻辑，比如用来判断主从，使用它可能导致其它问题</li>
<li>异常处理机制不同，施加全局锁后如果发生异常（如客户端断开），全局锁会被自动释放；而 readonly 只能主动解除</li>
</ol>
<h1 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h1><p>MySQL 里面表级别的锁有两种：表锁和元数据锁（meta data lock，MDL)。</p>
<h2 id="表锁"><a href="#表锁" class="headerlink" title="表锁"></a>表锁</h2><p>表锁是 MySQL server 层实现的锁。</p>
<p>表锁的语法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lock tables t_a read/write;</span><br><span class="line">lock tables t_a read/write, t_b read/write;</span><br><span class="line"></span><br><span class="line">unlock tables</span><br></pre></td></tr></table></figure>

<p>与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。</p>
<p>当某个事务需要查询某个表的大部分或全部数据时，如果使用更细的行锁会导致查询效率更慢，或者判断容易引发长时间锁等待&#x2F;死锁等，可以考虑使用表锁。</p>
<p>表级锁不会死锁。因为两个前提：</p>
<ol>
<li>需要的表锁只能一次性申请，不允许分多次执行 lock table t_a read(write) 和 lock table t_b read(write)</li>
<li>施加表锁成功除了会限制别的线程的读写外，也限定了本线程接下来只能操作申请了锁的表</li>
</ol>
<h2 id="元数据锁"><a href="#元数据锁" class="headerlink" title="元数据锁"></a>元数据锁</h2><blockquote>
<p>解决什么问题、使用方式、DDL 的正确方式</p>
</blockquote>
<p>MySQL 中数据定义语言（Data Definition Language - 缩写 DDL） 不属于事务范畴，如果 DDL 和事务同时修改某个表时可能破坏事务特性等，为了解决这类问题，MySQL 5.5.3 引入了元数据锁（Meta Data Lock），缩写 MDL。</p>
<p>MDL 的作用是解决同一张表上事务和 DDL 并行执行可能破坏一致性的问题。例如一个查询正在遍历一个表中的数据，而执行期间另一个线程删除了该表某一列，就会导致查询线程拿到的结果跟表结构对不上</p>
<p>MDL 不需要显式使用，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。</p>
<p>MySQL 5.6 支持了 online DDL。</p>
<p>MDL 会直到事务提交才释放，所以 DBA 要尽量在闲时在做表结构变更，而开发则应尽量保证事务效率，避免大事务。否则容易引发大规模锁等待。</p>
<p>问：备份一般都会在从库上执行，在用 –single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表执行了一个 DDL 操作，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？</p>
<p>答：备份的时候主库对 a 表做DDL，假设 DDL 未执行完，a  表上有 MDL 写锁，除了当前线程以外获取不到表结构修改后的结果。此时从库和备库均看不到 DDL 操作结果。即使主库 DDL 执行完毕，备份还在进行，因为备份时从库会启动全局锁，从库处于只读状态，从库依然看不到 DDL 的操作结果。</p>
<h3 id="安全的-DDL-方式"><a href="#安全的-DDL-方式" class="headerlink" title="安全的 DDL 方式"></a>安全的 DDL 方式</h3><p>如果要执行 DDL 的表是热点表，虽然数据量不大，但是增删改查很频繁，而我们不得不执行 DDL 操作，怎么做比较合适呢？</p>
<p>鉴于高频使用，此时手动 kill 已不管用。比较理想的机制是，在 alter table 语句里面设定等待时间，如果当前线程在等待时间内能够拿到 MDL 写锁则问题解决，拿不到也不会阻塞正常业务。之后开发人员或者 DBA 再通过重试命令重复这个过程。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ALTER TABLE tbl_name NOWAIT add column ...</span><br><span class="line"></span><br><span class="line">ALTER TABLE tbl_name WAIT N add column ...</span><br></pre></td></tr></table></figure>



<p>如果遇到大表加字段呢</p>
<p>pt-online-schema-change 是 percona 开发的 MySQL 管理工具之一。它的流程跟手动差不多，先新建一张与源表相同结构的表，执行 DDL 操作，再将数据拷贝到新表直到追上最新数据，然后 rename 表替换源表，最后删除源表。好处是执行过程中几乎不影响数据库性能，非常有意思。</p>
<p>安装：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget https://downloads.percona.com/downloads/percona-toolkit/3.3.1/binary/redhat/8/x86_64/percona-toolkit-3.3.1-1.el8.x86_64.rpm</span><br><span class="line">sudo yum -y install perl-DBI perl-DBD-MySQL perl-Digest-MD5 perl-IO-Socket-SSL perl-TermReadKey</span><br><span class="line">sudo rpm -ivh percona-toolkit-3.3.1-1.el8.x86_64.rpm</span><br><span class="line"># 验证是否安装成功</span><br><span class="line">pt-online-schema-change --help</span><br></pre></td></tr></table></figure>

<p>下面是常见执行示例：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pt-online-schema-change --user &#x27;xxx&#x27; --password &#x27;abcxxx&#x27; -P3313 -S /tmp/mysql3313.sock -D database_name --alter &quot;add column column_a int(11) unsigned DEFAULT 0 COMMENT &#x27;666&#x27; AFTER id;&quot; A=utf8,D=database_name,h=localhost,t=&#x27;table_name&#x27; --print --statistics --execute</span><br></pre></td></tr></table></figure>

<p>详见<a href="https://docs.percona.com/percona-toolkit/pt-online-schema-change.html">官方文档</a></p>
<h1 id="自增锁"><a href="#自增锁" class="headerlink" title="自增锁"></a>自增锁</h1><p>自增锁是 InnoDB 是在 auto_increment 时保证该值正确性的锁。</p>
<p>自增锁是执行完立刻就释放资源的锁！</p>
<h1 id="行锁"><a href="#行锁" class="headerlink" title="行锁"></a>行锁</h1><p>MySQL 的行锁由存储引擎自主实现，不是所有引擎都支持行锁，例如 MyISAM。</p>
<p>行锁因为涉及 MVCC 数据快照等保存操作，开销比表锁大，但粒度小</p>
<blockquote>
<p>注意：Innodb 行锁锁的是索引， 如果更新、删除条件没走索引， 会降级成表锁</p>
</blockquote>
<h2 id="两阶段锁"><a href="#两阶段锁" class="headerlink" title="两阶段锁"></a>两阶段锁</h2><blockquote>
<p>定义、目的、优化</p>
</blockquote>
<p>在 InnoDB 事务中，行锁都是两阶段锁：</p>
<ol>
<li>锁在需要的时候才施加</li>
<li>锁的申请和释放分步进行，中间不允许交叉申请和释放锁。</li>
<li>事务提交的时候释放锁资源</li>
</ol>
<p>因为行锁具有【锁是在需要的时候才施加的】的特性，所以如果事务需要锁多个数据行，将最可能造成锁冲突、最可能影响并发度的锁尽量往后放。可以减少锁占用时长，降低死锁几率。</p>
<h1 id="读锁和写锁"><a href="#读锁和写锁" class="headerlink" title="读锁和写锁"></a>读锁和写锁</h1><p>读锁和写锁是从另一个视角看待锁定义的，读写锁施加在行级数据时又叫行锁，施加到表级数据时又叫表锁。</p>
<p>读锁又叫共享锁，大家都能读，大家都不能写。自身写会立刻报错，别的线程写则会锁等待。</p>
<p>写锁又叫排它锁，只有我能读写，其他人都不能读写。别的线程读写会锁等待。</p>
<h1 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h1><blockquote>
<p>处理策略、</p>
</blockquote>
<p>当并发系统中多个线程出现循环锁等待，称为死锁。</p>
<p>死锁可以优化但几乎无法杜绝。</p>
<h2 id="死锁处理策略"><a href="#死锁处理策略" class="headerlink" title="死锁处理策略"></a>死锁处理策略</h2><p>当出现死锁以后，有两种主流策略：</p>
<p>一种策略是超时退出。死锁时一直等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 设置。弊端是如果 innodb_lock_wait_timeout 设置很大在实际业务场景中是不可接受的，如果设置很小则容易误伤只是正常锁等待的语句。</p>
<p>另一种策略是死锁检测。发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个模块。</p>
<p>死锁监测是常用的策略。</p>
<p>但是死锁监测需要耗费大量的 CPU 资源。整体死锁检测的耗费代价是 O(n^2) 级别。 假设并发更新同一行的 1000 个线程，整体耗费的死锁检测操作为 1000*1000&#x3D;100 万。 估算如下：</p>
<p>并发更新行 R1 的某线程 Tx，其所作的死锁检测工作为，Tx 先查看自身持有的行锁，同时遍历其他 999 个线程持有的行锁，共 1+999&#x3D;1000 次。x 为 1000。</p>
<p>为什么需要遍历所有其它线程（999 个），而不是仅看当前持有 R1 行锁的线程就行了呢？—— 因为行锁排队，此时此刻没有构成死锁闭环不代表以后不会。假设某线程 Tm 排队等待 R1 行锁，并排在 Tx 前。如果 Tx 当前持有行锁 R2，可能存在过一会 Tm 先于 Tx 获持 R1，事态变成【Tm 持有 R1，等待 R2 &amp;&amp; Tx 持有 R2，等待 R1，也就是 Tm 和 Tx 构成死锁闭环】。 因此并发更新同一行的有 N 个线程都需要被检测，资源损耗 O(N^2) 。</p>
<p>死锁检测不可避免，为防止死锁检测代价过高引起性能问题，应该想办法减少更新同一行记录的并发度，即降低 N 值。</p>
<p>优化方法：</p>
<ol>
<li>如果能确保业务不会出现死锁，可以临时把死锁监测关掉。</li>
<li>控制并发度。基本思路是，对于相同行的更新在进入引擎之前排队，这样 innodb 内部就不会有大量的死锁监测工作了。</li>
<li>从业务层面优化，例如做到将一行拆成多行的方式存储，以此减少行锁竞争。</li>
</ol>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL索引</title>
    <url>/2023/02/43a71ae4/</url>
    <content><![CDATA[<h1 id="索引数据结构的演变"><a href="#索引数据结构的演变" class="headerlink" title="索引数据结构的演变"></a>索引数据结构的演变</h1><p>索引是为了提高查询效率组织的数据目录。其作为加速查询效率的工具，数据结构组成尤为重要，以下尝试分析不同算法结构对索引实现的影响。</p>
<span id="more"></span>

<p>description: MySQL 索引作为加速查询效率的工具，其组成数据结构尤为重要，以下尝试分析不同算法结构对索引实现的影响。</p>
<p>首先是非常常见的几种数据结构：哈希表、数组、二叉树，根据它们的特点可以比较容易分析出它们构建的数据库索引的优缺点。</p>
<h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><p>哈希表的思路很简单，使用 key-value 结构存储数据，通过哈希函数和 key 值计算 value 的存储位置，直接读取。</p>
<p>遇到多个 key 计算哈希值结果一致的情况，可以进一步迭代成 哈希表 + 数组的形式，将多个 value 以链表的形式存储，读取时遍历链表。</p>
<p>哈希表的特点是：</p>
<ol>
<li>利用哈希底层是数组，而数组随机访问快（O(1)）的特点加速查询效率，故等值查询很快。但是范围查询很慢，因为需要全表扫描</li>
<li>哈希表中各个元素之间是无序的，所以插入很快，只需要末尾直接添加即可</li>
</ol>
<h2 id="有序数组"><a href="#有序数组" class="headerlink" title="有序数组"></a>有序数组</h2><p>有序数组在等值查询和范围查询的效率都很高（二分法，效率 O(logN)）但是插入效率很低。因为往某位置插入数据时，需要移动后面所有的数组元素。</p>
<p>因为插入效率慢，所以有序数组只适合静态数据的存储引擎</p>
<h2 id="平衡二叉查找树"><a href="#平衡二叉查找树" class="headerlink" title="平衡二叉查找树"></a>平衡二叉查找树</h2><p>平衡二叉查找树有以下特点：</p>
<ol>
<li>每个节点的值大于其左子节点值，小于右子节点值</li>
<li>最后一层叶子节点靠左排列</li>
<li>所有叶子节点高度差 &lt;&#x3D; 1</li>
</ol>
<p>得益于数据结构，构建的索引查询和插入更新的效率都是 O(logN)，是理想的效率模型。</p>
<p>但是平衡二叉查找树实际上依然有很大限制！因为二叉树每个节点只有 2 个子节点，同时每个节点只存储一条数据，假设某表有 100W 条记录，就是 100W 个叶子节点，此时树高为 20，那么每次等值查询需要访问 20 个节点数据进行判断，读写的 IO 消耗将非常高。同时二叉查找树存储也是个问题，占用的磁盘和内存空间都难以接受。</p>
<p>为了解决优化存储数据低效的问题，大佬们在平衡二叉查找树的基础上提出了 B 树。</p>
<h2 id="B-树"><a href="#B-树" class="headerlink" title="B 树"></a>B 树</h2><p>B 树是在平衡二叉查找树的基础上提出的，主要解决两个问题：</p>
<ol>
<li>二叉树的每个节点只有 2 个子节点</li>
</ol>
<p>解决：B 树是 N 叉树，根据特定规则（<a href="https://blog.csdn.net/shadow_2011/article/details/122369071">B+索引树插入记录操作讲解</a>）在插入等操作时进行节点分裂、合并，总体上维持较好的平衡，同时 N 叉树导致树高大大降低。</p>
<ol start="2">
<li>二叉树每个节点只能存储一个数据</li>
</ol>
<p>解决：B 树每个节点同时保存多个元素，此时每个节点对应磁盘上的磁盘块，也就是说根据磁盘块来组织 B 树的节点存储。这样也主动适应了【磁盘读取每次都是读取一个磁盘块数据】的特性。</p>
<blockquote>
<p>事实上节点的大小也可能是磁盘块大小的整数倍，不过不影响从索引的角度理解其特性</p>
</blockquote>
<p>B 树优化了很多磁盘操作层面的问题，不过依然不尽如人意，原因是在每个节点对应一个磁盘块的情况下，受限于磁盘块大小固定（4k 或 16k），每个节点（磁盘块）能同时存储的节点指针和数据信息有限，导致 N 叉树的 N 还是没有很大，也就是说树高还是蛮高的。由此数据库存储引擎做了进一步的算法优化，就有了 B+ 树。</p>
<h2 id="B-树-1"><a href="#B-树-1" class="headerlink" title="B+ 树"></a>B+ 树</h2><p>B+ 树在 B 树的基础上进一步优化节点的存储问题，B+ 树只有叶子节点存储数据，非叶子节点只存储键值和指针。同时 B+ 树的数据是顺序存储，提高了范围查询、排序查询的效率。</p>
<h3 id="B-树叶子节点的存储"><a href="#B-树叶子节点的存储" class="headerlink" title="B+树叶子节点的存储"></a>B+树叶子节点的存储</h3><p>B+ 树约定仅在叶子节点存储数据，在每个磁盘数据块大小为 16K 的条件下大约可以做到 1000-1200 叉树，此时存储 10 亿条键值数据层高只有 3，意味着只需要 3 次磁盘 IO 即可得到结果（事实上根节点通常常驻内存，只需要 2 次 IO）</p>
<p>1200 叉树的计算：</p>
<p>目前常见磁盘每个磁盘块大小约 16k，以常见主键索引类型 int 计算：</p>
<p>int 类型占用磁盘空间为 4 字节，指针大小在 InnoDB 中统一设置为 6 字节，一共 10 字节空间。也就是一个磁盘块允许存储 16 * 1024 &#x2F; 10，约 1638 个主键及其指针。假设每个指针指向数据行大小平均为 1k，那么一个树高为 2 的 B+ 数可以存储 1638 * 16 &#x3D; 26208 条记录，树高为 3 的 B+ 树可以存储 1638 * 1638 * 16 &#x3D; 42928704 条记录，轻松满足 4kw 记录存储。</p>
<p>同理，bigint 主键索引时也能满足千万级别的数据存储。</p>
<h3 id="B-树叶子节点的双向链表"><a href="#B-树叶子节点的双向链表" class="headerlink" title="B+树叶子节点的双向链表"></a>B+树叶子节点的双向链表</h3><p>同时，B+ 树通过在叶子节点将构建双向链表，叶子节点内部的数据构建单向链表，满足范围查询和顺序查询的效率。</p>
<p>B+ 树因为读写性能优秀，同时适配磁盘的访问模式，已经广泛应用在数据库引擎的存储结构上。不同存储引擎对 B+ 树的实现、使用有细微差别，例如 MyISAM 中 B+ 数的叶子节点不直接存储数据，而是存储数据的文件地址，innodb 则是直接存储数据。</p>
<blockquote>
<p>注意区分 [innodb 则是直接存储数据] 的角度理解，在 innodb 的非主键索引树中，”数据”指的是主键索引值，主键索引树时 “数据” 则是具体的数据行。</p>
</blockquote>
<h1 id="innodb-的索引模型"><a href="#innodb-的索引模型" class="headerlink" title="innodb 的索引模型"></a>innodb 的索引模型</h1><p>innodb 中每个数据表有一棵主键索引树和若干棵非主键索引树，数据结构均默认为 B+ 树。</p>
<p>主键索引树：innodb 的主键索引树的叶子节点存储的是具体的行数据，非叶子节点则存储主键值。</p>
<p>非主键索引树：叶子节点存储主键值，查询时先在非主键索引树查询对应的主键值，再到主键索引树查询行数据，这个过程叫回表。</p>
<p>查询时，依次优先使用主键索引树 &gt; 非主键索引树 &gt; 不使用索引（直接遍历主键索引树）</p>
<p>主键索引又叫聚簇索引，非主键索引又叫二级索引（非聚簇索引）。</p>
<h2 id="innodb-索引维护"><a href="#innodb-索引维护" class="headerlink" title="innodb 索引维护"></a>innodb 索引维护</h2><p>因为 B+ 树本身维护了索引的有序性，那么就需要考虑增删对索引树结构的影响了，因为插入和删除记录都可能引发索引树叶子节点分裂或合并。比如插入新记录时发现某个磁盘块已写满，此时就需要组织新磁盘块并将原磁盘块中部分数据迁移过去，体现在 B+ 树上就是裂变出新的叶子节点。</p>
<p>innodb 为了优雅地解决索引树叶子结点的维护问题，使用了以下办法：</p>
<ol>
<li><p>提倡使用独立的自增值作为表主键</p>
</li>
<li><p>删除时仅逻辑删除而不是物理删除</p>
</li>
</ol>
<p>使用独立的自增值作为表主键使得主键索引树在插入新记录时总是【追加】操作，几乎不费力就能维护树的顺序结构，同时自增值代替业务字段可以节省存储空间。</p>
<p>删除操作时仅对记录添加删除标记，搜索引擎统一处理页合并。因为是软删除所以单次操作不需要考虑页合并问题，提高了操作速度。</p>
<h3 id="使用业务字段作为主键的坏处"><a href="#使用业务字段作为主键的坏处" class="headerlink" title="使用业务字段作为主键的坏处"></a>使用业务字段作为主键的坏处</h3><ol>
<li>业务字段长度不可控而且可能更长，容易导致非主键索引树的存储需要更大空间（叶子结点的数据是主键值）</li>
<li>业务字段通常不是递增的，会导致新增主键值在主键索引树不再是追加，为了维持索引树的有序性，搜索引擎被迫执行大量页分裂，导致整体性能下降。</li>
</ol>
<h2 id="重建索引"><a href="#重建索引" class="headerlink" title="重建索引"></a>重建索引</h2><p>重建普通索引树很简单：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table table_a drop index k;</span><br><span class="line"></span><br><span class="line">alter table table_a add index k(`k`);</span><br></pre></td></tr></table></figure>

<p>通过重建索引会得到一个全新的索引树，优化旧索引树因为页分裂等导致的数据空洞问题，提高数据页的使用率。</p>
<p>但是重建主键索引树应该倍加谨慎，如果删除主键列&#x2F;主键索引，MySQL 会使用内置的 RowID 代替原主键构建索引树（包括二级索引树），表对应的索引树都面临重新构建的需要。</p>
<p>正确的做法是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">alter table table_a engine=InnoDB;</span><br></pre></td></tr></table></figure>



<h2 id="索引树的使用"><a href="#索引树的使用" class="headerlink" title="索引树的使用"></a>索引树的使用</h2><p>使用索引树查询时，查询引擎会对数据挨个进行搜索，直到获得不符合要求的数据停止。</p>
<p>借用 MySQL 45 讲大佬的举例解释一下，前置准备：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table T (</span><br><span class="line">    ID int primary key,</span><br><span class="line">    k int NOT NULL DEFAULT 0,</span><br><span class="line">    s varchar(16) NOT NULL DEFAULT &#x27;&#x27;,</span><br><span class="line">	index k(k)</span><br><span class="line">) engine=InnoDB;</span><br><span class="line"></span><br><span class="line">insert into T values(100,1, &#x27;aa&#x27;),(200,2,&#x27;bb&#x27;),(300,3,&#x27;cc&#x27;),(500,5,&#x27;ee&#x27;),(600,6,&#x27;ff&#x27;),(700,7,&#x27;gg&#x27;);</span><br></pre></td></tr></table></figure>

<p>当执行语句 select * from T where k between 3 and 5; 时，执行过程如下：</p>
<ol>
<li>搜索 k 索引树查询得到 k&#x3D;3 记录，获得 ID&#x3D;300</li>
<li>到主键索引树搜索 ID&#x3D;300 的记录，返回数据给执行器</li>
<li>通过磁盘指针，继续查询 k 索引树中的数据，得到 k&#x3D;5 对应 ID&#x3D;500（注意这里不需要重新遍历 k 索引树，因为索引是有序的）</li>
<li>再次回表，搜索主键索引树得到 ID&#x3D;500 航记录，返回数据给执行器</li>
<li>重复 3 步骤，得到下一个值 k&#x3D;6，不满足查询条件，查询结束</li>
</ol>
<p>综上，查询的过程中查询了 k 索引树的 3 条记录（binlog 中的 row_examined 是 3），回表了 2 次。</p>
<p>如果查询语句是 select ID from T where k between 3 and 5，因为 k 索引树可以直接得到 ID 值，此时就不需要回表了。此时我们叫 k 索引为覆盖索引。</p>
<h2 id="覆盖索引和冗余索引"><a href="#覆盖索引和冗余索引" class="headerlink" title="覆盖索引和冗余索引"></a>覆盖索引和冗余索引</h2><p>覆盖索引：当一个索引树在执行某查询时不需要执行回表，则称该索引为覆盖索引，因为不需要回表故表现为效率提升。</p>
<p>冗余索引：当多个联合索引的某前置列相同时，这几个联合索引互为冗余索引。</p>
<p>上述只是为了描述某个情景的说法，不需要纠结。事实上，覆盖索引和冗余索引的设置是矛盾的过程，总得来说就是综合考量，根据实际情况使用。</p>
<p>假设有两列，比如”公民身份证号”和”姓名”，虽然姓名一般看上去不需要加入索引，但是事情不是绝对，如果有高频请求总是查询这两个字段建立覆盖索引能显著加速则是 OK 的；相应的，如果有几个查询都用到了”身份证号”和其它字段，而且频率不高，无脑构建覆盖索引反而导致资源浪费（因为联合索引本身就意味着更大的存储空间和维护成本）。</p>
<p>所以一般从几个方面综合考虑：</p>
<ol>
<li>查询的频率</li>
<li>组织覆盖索引造成的资源使用问题</li>
</ol>
<h2 id="最左前缀原则"><a href="#最左前缀原则" class="headerlink" title="最左前缀原则"></a>最左前缀原则</h2><p>为了避免创建多个索引，B+ 树支持最左前缀原则，即多个字段的联合索引可以允许前 N 个字段索引生效。原因是存储引擎构建联合索引树对索引值排序时，先根据第一个字段排序，当第一个字段值相同时再对第二个字段排序，以此类推构成有序索引，再存储到索引树的非叶子节点上。联合索引的字段（和字符）按照定义顺序，从左到右依次生效</p>
<p>注：最左前缀原则不仅仅指联合索引的最左 N 个字段，也可以是某个字符串字段的左边 M 个字符。</p>
<h2 id="索引下推"><a href="#索引下推" class="headerlink" title="索引下推"></a>索引下推</h2><p>MySQL &gt;&#x3D; 5.6 搜索联合索引树时，会优先判断索引字段是否符合查询条件，不符合则直接跳过，目的是减少回表次数。</p>
<p>以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">select * from tuser where name like &#x27;张%&#x27; and age=10 and ismale=1;</span><br></pre></td></tr></table></figure>

<p>根据最左前缀原则，MySQL 会一直向右匹配索引，直到遇到范围查询（&gt;、&lt;、between、like）就停止。所以这条查询只会使用到 name 字段的索引。</p>
<p>在 MySQL &lt; 5.6 时， 此时会根据 name 字段索引筛选得到的主键值回表，取出行记录中的 age 和 ismale 进行判断，返回符合判断条件的记录。</p>
<p>MySQL &gt;&#x3D;5.6 进一步优化，因为有索引（name，age）存在，将根据联合索引最左前缀原则拿到的索引信息，先判断索引中包含的字段是否符合条件（这里是 age，判断是否等于 10），符合时才会回表，以此减少回表次数，这就叫索引下推。</p>
<h1 id="附"><a href="#附" class="headerlink" title="附"></a>附</h1><p><a href="https://blog.csdn.net/shadow_2011/article/details/122369071">B+索引树插入记录操作讲解</a></p>
]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
</search>
